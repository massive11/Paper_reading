>论文标题：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift  
发表时间：2015  
研究组织：Google  
本文标签：批标准化、深度学习、CNN、ICML


# 速读概览：
## 1.针对什么问题？ 
    深度神经网络在训练过程中，每一个层的输入分布会因为先前层的参数发生变化而改变，需要降低学习率并谨慎的初始化参数，这会减慢训练速度。
## 2.采用什么方法？  
    在模型架构中添加批标准化层，并对每次训练的小批量进行标准化处理，使得深度神经网络训练过程中每一层神经网络的输入保持相同分布。
## 3.达到什么效果？  
    在训练步骤少14倍的情况下实现了相同的精度，并以显著优势击败了原始模型，超过了人工分类器的准确性。
## 4.存在什么不足？
    


# 论文精读
## 0.摘要
* 训练深度神经网络的复杂性在于随着先前的层的参数发生变化，每个层的输入分布在训练过程也会发生改变。由于需要较低的学习率和仔细的参数初始化，这会减慢训练速度，并且使得训练具有饱和非线性的模型变得非常困难。我们将这种变化称为内部协变量转移，并通过标准化层输入来解决问题。我们的方法通过添加标准化到模型架构并对每个训练的mini批实施标准化中汲取力量。批标准化允许我们使用更高的学习率并且不需要对初始化如此谨慎。它还能起到正则化的作用，在某些情况下，消除了dropout的需要。批标准化应用在最新的图像分类模型上，在训练步骤少14倍的情况下实现了相同的精度，并以显著优势击败了原始模型。使用批标准化网络组合，我们改进了ImageNet分类的最佳发布结果：达到4.9%的前五名验证误差（4.8%的测试误差），超过了人工分类器的准确性。

## 1.Introduction
* 使用小批量样本相比于一次处理单个样例在以下几方面是有用的
  * 一次小批量上损失的梯度是整个训练集上梯度的估计，随着batch的size的提升，其质量也会提高
  * 由于现代计算平台的并行性，在一个batch上的计算比对单个样例的m次计算要更加高效
* 随机梯度虽然简单高效，但需要仔细的调整模型超参数，特别是优化中的学习率和模型参数的初始值。每一层的输入都会受到所有处理中的层的影响，随着网络加深，这种网络参数的细小变化也会被放大。
* 层输入的分布变化是一个问题，因为这些层需要不断适应新的分布。当学习系统的输入分布发生变化时，会经历协变量转移。这通常是通过域适应解决的。但是协变量转移的概念会拓展到学习系统之外。