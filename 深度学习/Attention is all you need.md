>论文标题：Attention is all you need  
发表时间：2017  
研究组织：Google  
本文标签：网络结构、Transformer、NIPS


# 速读概览：
## 1.针对什么问题？ 

## 2.采用什么方法？  

## 3.达到什么效果？  

## 4.存在什么不足？



# 论文精读
## 0.摘要
* 主流的序列转导模型都是基于包含一个编码器和一个解码器的复杂的循环或卷积神经网络。性能最好的模型也是通过注意力机制将编码器和解码器连接起来。我们提出了一个新的简单的网络架构，Transformer，它完全基于注意力机制，完全消除了递归和卷积。在两个机器翻译任务上的实验表明这些模型在质量上更胜一筹，同时更具有并行性，并且需要更少的训练时间。我们的模型在WMT 2014 英-德翻译任务上取得了28.4 BLEU，比现有的最佳结果（包括集成）提高了 2 BLEU。 在 WMT 2014 英语到法语翻译任务中，我们的模型在 8 个 GPU 上训练 3.5 天后建立了一个新的单模型最先进的 BLEU 分数 41.8，这个cost只是最佳模型训练成本的一小部分。 我们还展示了通过将 Transformer 成功应用于具有大量和有限训练数据的英语选区解析，可以很好地推广到其他任务。

## 1.Introduction
* 循环神经网络，特别是长短期记忆和门控循环神经网络，已被牢固地确立为序列建模和转导问题（如语言建模和机器翻译）的最先进方法。此后，无数努力继续推动循环语言模型和编码器-解码器架构的界限。
* 循环模型通常沿着输入和输出序列的符号位置进行因子计算。