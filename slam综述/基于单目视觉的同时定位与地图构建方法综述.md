>论文标题：基于单目视觉的同时定位与地图构建方法综述  
发表时间：2016  
研究组织：浙江大学  
本文标签：SLAM、CCF-A  


# 速读概览：
## 主要内容
V-SLAM综述


## 0.摘要
本文首先介绍了基于视觉的SLAM的基本原理，然后介绍了几个代表性的基于单目SLAM的方法并做深入分析和比较。


## 0.5.导入部分
* GPS无法在室内及遮挡严重的室外环境中使用，且定位精度较低；高精度的惯导系统成本太高且难以民用；基于无线信号的定位方法需要事先布置使用场景等。而基于视觉的SLAM硬件成本低廉、小场景范围内精度较高、无需预先布置场景等，逐渐成为常采用的定位方案。
* SLAM最早源于机器人领域，目标是**在一个未知的环境中实时重建环境的三维结构并同时对机器人进行定位**。在计算机视觉领域，与之类似的技术是运动推断结构（SFM）。


## 1.V-SLAM的基本原理
* V-SLAM技术可以**根据拍摄的视频信息推断出摄像头在未知环境中的方位, 并同时构建环境地图**, 其基本原理为多视图几何原理。
* V-SLAM的目标为同时恢复出每帧图像对应的相机运动参数$C_1...C_m$, 及场景三维结构$X_1...X_n$。每个相机运动参数$C_i$包含了相机的位置和朝向信息, 通常表达为一个3×3的旋转矩阵$R_i$和一个三维位置变量$P_i$。$R_i$,$P_i$将一个世界坐
标系下的三维点$X_j$变换至$C_i$的局部坐标系
$$(X_{ij},Y_{ij},Z_{ij})^T = R_i(X_j)-p_i \tag{1}$$
进而投影到图像中
$$h_{ij} = (f_xX_{ij}/Z_{ij}+c_x,f_yY_{ij}/Z_{ij}+c_y)^T \tag{2}$$
其中，$f_x$和$f_y$分别为沿图像x，y轴的图像焦距，$(c_x,c_y)$为镜头光心在图像中的位置，通常假设这些参数已事先标定且保持不变。由(1)(2)可知，三维点在图像中的投影位置$h_{ij}$可表示为一个关于$C_i$和$X_j$的函数，记为
$$h_{ij} = h(C_i,X_j) \tag{3}$$
* V-SLAM算法需要将不同图像中对应于相同场景点的图像点匹配起来，通过求解优化如下目标函数
$$\mathop {argmin}_{C_1...C_m,X_1...X_n} \sum_{i=1}^m\sum_{j=1}^n \left \| h(C_i,X_j)-\hat x_{ij} \right \|_{\sum_{ij}} \tag{4}$$
得到一组最优的$C_1...C_m$,$X_1...X_n$，使得所有$X_j$在$C_i$图像中的投影位置$h_{ij}$与观测到的图像点位置$x_{ij}$尽可能靠近。假设图像观测点符合高斯分布$x_{ij}\sim N(\hat x_{ij},\sum_{ij})$,$\left \| e \right \|_{\sum} = e^T\sum ^{-1}e$。求解目标函数(4)的过程也称为集束调整（BA），可利用线性方程的稀疏结构高效求解。
* 由于V-SLAM需要进行图像特征的匹配，因此其稳定性严重依赖于特征的丰富程度。可以通过在V-SLAM中加入其他传感器信息解决这一问题。目前最常用的是在V-SLAM中结合IMU数据（加速度、角速度）。这样的SLAM称为VIN或VI-SLAM。将相邻2帧间的所有IMU数据标记为集合$Z_i = {z_1...z_{n_i}}$，VI-SLAM方法一般求解优化如下目标函数
$$\mathop {argmin}_{C_1...C_m,X_1...X_n} \sum_{i=1}^m\sum_{j=1}^n \left \| h(C_i,X_j)-\hat x_{ij} \right \|_{\sum_{ij}}+\sum _{i=1}^{m-1}\left \| f(C_i,Z_i)-C_{i+1} \right \|_{\Gamma _i} \tag{5}$$
与(4)相比，VI-SLAM引入了一个运动方程，其中$f(C_i,Z_i)$为$Z_i$作用于$C_i$后的运动参数，$\Gamma _i$为运动方程的协方差矩阵。通常V-SLAM需求解每一时刻的运动速度$v_i$和IMU数据的偏移量$b_i$，即$C_i = (R_i,p_i,v_i,b_i)$。
* 类似的原理可应用于其他传感器数据，如引入GPS数据$p_i^G$，只需在能量函数中再引入一项
$$\mathop {argmin}_{C_1...C_m,X_1...X_n} \sum_{i=1}^m\sum_{j=1}^n \left \| h(C_i,X_j)-\hat x_{ij} \right \|_{\sum_{ij}}+\sum _{i=1}^{m-1}\left \| f(C_i,Z_i)-C_{i+1} \right \|_{\Gamma _i}+ \sum_{i=1}^{m-1}\left \| p_i-\hat p_i^G\right \|_{\Lambda_i} \tag{6}$$
此处，假设GPS观测值符合高斯分布
$p_i^G \sim N(\hat p_i^G,\Lambda_i)$

## 2.代表性单目V-SLAM系统
* 主流的V-SLAM方法大致可以分为3类：基于滤波器、基于关键帧BA和基于直接跟踪的V-SLAM。
### A.基于滤波器的V-SLAM
* 思想：将每一时刻t的系统状态用一个高斯概率模型表达，$x_t\sim N(\hat x_t ,P_t)$，$\hat x_t$为当前时刻系统状态估计值，$P_t$为该估计值误差的协方差矩阵。系统状态由一滤波器不断更新。不同的状态设计和滤波方式衍生出不同的SLAM系统。
* MonoSLAM是第一个成功基于单目摄像头的纯视觉SLAM系统。MonoSLAM的状态$x_t$由t时刻的相机运动参数$C_t$和所有三维点位置$X_1...X_n$构成，每一时刻的相机方位均带有一个概率偏差。同样，每个三维点位置也带有一个概率偏差，可以用一个三维椭球表示，椭球中心为估计值，椭球体积表示不确定成都；不同场景点之间，以及场景点和$C_t$之间均有概率关联。在此概率模型下，场景点投影至图像的形状为一个投影概率椭圆。MonoSLAM 为每帧图像中抽取 Shi-Tomasi 角点, 在投影椭圆中主动搜索(active search)特征点匹配。
  * MonoSLAM的滤波器选用的是扩展卡尔曼滤波器。在预测阶段，采用运动方程
$$C_t = f(C_{t-1},a_v\Delta t,a_\omega \Delta t) \tag{7}$$
其中，$a_v$和$a_\omega$分别为线性和旋转加速度，算法假设$a_v \sim N(0,\Gamma _v)$，$a_\omega \sim N(0,\Gamma_\omega)$，$\Delta t$为相邻2帧时间差。在更新阶段，采用投影方程
$$\hat x_j = h(C_t,X_j)+n_j \tag{8}$$
其中，$\hat x_j$为当前帧观测到$X_j$的图像点位置，$n_j \sim N(0,\sum_j)$。与(5)相比，MonoSLAM每一时刻只需估计当前时刻状态$x_t$，之前所有时刻的相机运动参数${C_1...C_{t-1}}$全部不参与计算，由此减少计算量。

* 传统 EKF 方法最主要的局限性如下:
  * 如果预测函数和更新函数为非线性(通常 V-SLAM 问题都是非线性的), 那么 EKF 并不能保证全局最优, 与 Levenberg-Marquardt等迭代的非线性优化技术相比, 更容易造成误差累积. 如果上一帧处理完成时刻的估计值尚未精确, 传递至下一帧的先验信息便带有误差; 由于上一帧状态不再变化, 所以先验信息中的误差便无法消除, 误差不断向后传递造成误差累积.
  * 若将三维点引入状态变量, 则每一时刻的计算复杂度为O(n3), 因此只能处理几百个点的小场景.
* 为了缓解 EKF 方法的计算复杂度问题, 2007年提出了 MSCKF. MSCKF 是一个 VI-SLAM 方法, 在预测阶段, 使用 IMU 数据进行传递系统状态; 在更新阶段,MSCKF将邻近的 l 帧相机运动参数全部包含进一个状态变量集合$C={C_{t-l+1}...C_t}$. C中每个$C_i$的估计值均在不断优化, 通常移出 C 前$C_i$已较为精确, 由此缓解误差累积. 除此之外, MSCKF 对所有三维点进行消元, 将$C_i$与$X_j$间的二元约束转换为$C={C_{t-l+1}...C_t}$间的多元约束, 从而将O(n3) 的计算复杂度简化为 O(nl3) . 因为一般来说$l<<n$, 而且为常数, 所以该方法可以大大降低计算复杂度, 将原来的跟三维点数成立方关系降到了线性关系.

### B.基于关键帧BA的V-SLAM
* PTAM是实时SFM系统，也是首个基于关键帧BA的单目V-SLAM系统。其思想为：将相机跟踪和地图构建作为 2 个独立的任务在 2 个线程并行执行. 地图构建线程仅维护原视频流中稀疏抽取的关键帧及关键帧中可见的三维点, 这样就可以非常高效地求解目标函数(4)(即 BA); 有了 BA 恢复的精确三维结构, 相机跟踪线程作为前台线程, 仅需优化当前帧运动参数$C_t$,足以达到实时的计算效率.
  * 前台线程通过一个匀速运动模型预测当前帧方位, 以搜索地图中的三维点在当前 帧图像中对应的FAST角点, 并根据匹配关系优化当前帧方位。
  $$argmin_{C_t}\sum _{j=1}^nw_j \left \| h(C_t,X_j)-\hat x_j \right \|_{\sum_j} \tag{9}$$
  其中$w_j$是 Tukey 函数对应的权重, 用于缓解误匹配对结果的影响. 如果成功匹配点数不足(如因图像模糊、快速运动等), 则判断跟踪失败, 开始重定位——将当前帧与已有关键帧的缩略图进行比较, 选择最相似的关键帧作为当前帧方位的预测, 重复上述特征匹配和方位优化步骤. 如果跟踪成功, 判断$C_t$是否满足关键帧条件, 一旦符合, 系统将当前帧作为新的关键帧传递给后台构建地图. 后台线程沿极线匹配不同关键帧之间对应于相同场景点的图像特 征点, 通过三角化恢复这些场景点三维位置, 并对所有关键帧和三维点运行 BA, 恢复精确的三维地图。

* ORB-SLAM 是目前性能最好的单目 V-SLAM 系统之一。ORB-SLAM 基本延续了 PTAM 的算法框架, 但对框架中的大部分组件都做了改进, 归纳起来主要有 4 点:   
  * ORB-SLAM 选用了 ORB 特征, 基于 ORB 描述量的特征匹配和重定位, 都比 PTAM 具有更好的视角不变性. 此外, 新增三维点的特征匹配效率更高, 因此能更及时地扩展场景. 扩展场景及时与否决定了后续帧是否能稳定跟踪；
  * ORB-SLAM加入了循环回路的检测和闭合机制, 以消除误差累积. 系统采用与重定位相同的方法来检测回路(匹配回路两侧关键帧上的公共点), 通过方位图优化来闭合回路. 关键帧作为方位图的节点, 每个关键帧被赋予一个相似变换$\xi _i$以矫正其方位; 方位图的边表示关键帧之间存在特征匹配，2团匹配的三维点云通过坐标对齐可求解一个相似变换$\xi _{ij}$；系统选用g2o优化方位图，以闭合回路。
$$argmin_{\xi_1,...,\xi_m}\sum_{(\xi_{ij},\sum _ {ij})}(\xi_{ij}\circ\xi_i^{-1}\circ\xi_j)^T \sum _{ij}^{-1}(\xi_{ij}\circ\xi_i^{-1}\circ\xi_j) \tag{10}$$
    其中，$\sum_{ij}$为$\xi_{ij}$的协方差矩阵，操作符$\circ$按顺序连接 2 个相似变换. 与全局 BA 相比, 方位图优化极大简化了全局优化的计算量, 因此 ORB-SLAM 能处理大尺度场景。BA 在系统中只运行于局部场景;
  * PTAM 需要用户指定 2 帧来初始化系统, 2 帧间既要有足够的公共点, 又要有足够的平移量. 平移运动为这些公共点提供视差, 只有足够的视差才能三角化出精确的三维位置. ORB-SLAM 通过检测视差来自动选择初始化的 2 帧;
  * PTAM 扩展场景时也要求新加入的关键帧提供足够的视差, 导致场景往往难以扩展. ORB-SLAM 采用一种更鲁棒的关键帧和三维点的选择机制——先用宽松的判断条件尽可能及时地加入新的关键帧和三维点, 以保证后续帧的鲁棒跟踪; 再用严格的判断条件删除冗余的关键帧和不稳定的三维点, 以保证 BA 的效率和精度。

### C.基于直接跟踪的V-SLAM
* 基于滤波器和基于关键帧 BA 的 V-SLAM 通常要在图像中提取并匹配特征点, 因此对环 境特征的丰富程度和图像质量(如模糊程度、图像噪声等)十分敏感. 相比之下, 直接跟踪法不依赖于特征点的提取和匹配, 而是直接通过比较像素颜色来求解相机运动, 因此通常在特征缺失、图像模糊等情况下有更好的鲁棒性。
* DTAM 是2011年提出的单目 V-SLAM 系统, 其最显著的特点是能实时恢复场景三维模型。基于三维模型, DTAM 既能允许 AR 应用中的虚拟物体与场景发生物理碰撞, 又能保证在特征缺失、图像模糊等情况下稳定地直接跟踪。具体来说, DTAM 预测一个与当前帧相机方位$C_t$十分接近的虚拟相机$C_v$, 在$C_v$下绘制场景三维模型, 由此求解$C_v$和$C_t$间的相对运动$\xi_{tv}$
$$argmin_{\xi_{tv}}\sum_{x\in \Omega_v}\left \| r(x,D_v(x),\xi_{tv}) \right \|_2^2 \tag{11}$$
  其中，$r(\bullet)$为颜色残差，
$$r(x,D_v(x),\xi_{tv}) = I_v(x)-I_t(\omega(x,D_v(x),\xi_{tv})) \tag{12}$$
  $I_v$和$D_v$分别为对三维模型在$C_v$下绘制得到的亮度图和深度图，$\Omega _v$为亮度和深度有效像素的集合，函数$\omega(x,D_v(x),\xi_{tv})$将虚拟帧v中的像素x投影至当前帧t中。
  为恢复场景三维模型, 后台需要持续恢复参考帧 r 的深度图$D_r$。这里选用的是逆深度方式表达深度。DTAM 将解空间离散为$M\times N\times S$的三维网格, 其中$M\times N$为图像分辨率，S为逆深度分辨率。DTAM选择参考帧r周围的后续帧$m\in N(r)$，对r中每个图像坐标为x、逆深度为$d=D_r(x)$的体素计算匹配代价
  $$C(x,d) = \frac 1{|N(r)|}\sum_{m\in N(r)} \left \| r(x,d,\xi_{mr}) \right \|_1  \tag{13}$$
  仅通过$argmin_d C(x,d)$恢复的深度图在无纹理区域存在歧义，因此，DTAM加入正则项
  $$R(x,d) = g(x)\left \| \nabla D_r(x) \right \|_\epsilon \tag{14}$$
  其中，$g(x)$为平滑权重，用于降低物体边界处的平滑程度
  $$g(x) = e^{-\alpha \left \| \nabla I_r(x) \right \|_2^\beta} \tag{15}$$
  $\left \| x \right \|_\epsilon$为Huber范数，
  $$\left \| x \right \|_\epsilon = \begin{cases}
\left \|x \right \|_2^2/2\epsilon, &if\ \left \|x \right \|_2\ \le\ \epsilon\\
\left \|x \right \|_1-\epsilon /2, &otherwise
\end{cases}  \tag{16}$$
  用于保留深度不连续区域。DTAM的能量函数为
  $$argmin_{D_r}\int (R(x,D_r(x))+\lambda C(x,D_r(x)))dx \tag{17}$$
  采用全变差技术求解。
  基于直接跟踪的 DTAM 对特征缺失、图像模糊有很好的鲁棒性, 但由于 DTAM 为每个像素都恢复稠密的深度图, 并且采用全局优化, 因此计算量很大, 即使采用 GPU 加速, 模型的扩展效率仍然较低。
* 与DTAM相比,LSD-SLAM仅恢复半稠密深度图, 且每个像素深度独立计算, 因此能达到很高的计算效率。LSD-SLAM 采用关键帧表达场景, 每个关键帧k包含图像$I_k$、逆深度图$D_k$和逆深度的方差$V_k$. 系统假设每个像素 x 的逆深度值服从高斯分布$N(D_k(x),V_k(x))$。LSD-SLAM 的前台线程采用直接跟踪法恢复当前帧 t 与关键帧 k 之间相对运动$\xi_{tk}$, 即求解优化式
$$argmin_{\xi}\sum_{x\in \Omega_k} \left \| \frac{r^2(x,D_k(x),\xi_{tk})}{\sigma_r^2(x,D_k(x),\xi_{tk})} \right \|_{\delta} \tag{18}$$
其中，$\Omega_k$是深度有效像素的集合；$\sigma^2(x,\xi)$为$r(\bullet)$的方差，用于减小深度误差对结果的影响，
$$\sigma_r^2(x,D_k(x),\xi_{tk}) = 2\sigma_I^2 + (\frac{\partial_r(x,D_k(x),\xi_{tk})}{\partial D_k(x)})^2V_k(x) \tag{19}$$
$\left \| \bullet \right \|_\delta$为阈值为$\delta$的Huber范数，用于缓解Outliers对结果的影响。LSD-SLAM 的后台线程对关键帧中每个半稠密抽取的像素点 x (梯度显著区域), 在$I_t$中沿极线搜索$I_k(x)$的对应点, 得到新的逆深度观测值$d_x$及其方差$\sigma_x^2$, 采用EKF更新$D_k$和$V_k$
$$\left \{ 
\begin{array}{c}
D_k(x) \leftarrow \frac{V_k(x)d_x+\sigma_x^2D(x)}{V_k(x)+\sigma_x^2}\\ 
V_k(x) \leftarrow \frac{V_k(x)\sigma_x^2}{V_k(x)+\sigma_x^2}
\end{array}
\right. \tag{20}$$
与 ORB-SLAM 类似, LSD-SLAM 也采用方位图优化, 因此能闭合循环回路和处理大尺度场景。系统为每个新加入的关键帧$k_i$选取距离最近的以及图像内容最相似的已有关键帧集合{$k_j$}, 采用直接跟踪法求解所有$(k_i,k_j)$间的相似变换$\xi_{ij}$及其协方差$\sum _{ij}$构造方位图并求解目标函数(10).
### D.分析和比较
* 定位精度。以 MonoSLAM 为代表的基于滤波器的 V-SLAM 由于容易在变量未精确时就进行消元,于是误差不断累积到下一帧, 因此精度劣于以PTAM为代表的基于关键帧 BA 的 V-SLAM。 MSCKF 虽然也基于滤波器, 但由于其推迟消元的机制, 加上融合了 IMU 信息, 通常能达到很高的精度. 比起同样基于关键帧 BA 的 PTAM, RDSLAM 和 ORB-SLAM 由于分别选用了匹配精度更高的 SIFT 特征和 ORB 特征, 而且采用了比 PTAM 更为高效的 BA 实现, 因此定位精度通常比 PTAM 更高; 而且 ORB-SLAM 可以通过闭合回路消除误差累积, 因此在大尺度场景下的定位精度最高. 基于直接跟踪法 DTAM 和 LSD-SLAM 对光照变化和动态干扰较为敏感, 因此精度一般会劣于 ORB-SLAM. LSD-SLAM 的定位误差比 ORB-SLAM 大 5~10 倍.
* 定位效率。MonoSLAM 的计算复杂度需要$O(n^3)$, 其中 n 为所有三维点个数, 因此定位效率较低. MSCKF 的计算复杂度为$O(nl^3)$, 其中 n 为跟踪轨迹结束于当前帧的三维点个数, l 为系统维护的局部帧数. MSCKF 的定位效率和精度之间存在权衡, 通常 l 越大精度越高, 但效率就越低, 反之亦然. PTAM 和 ORB-SLAM 的前台线程只需优化当前帧方位, 定位效率最高. RDSLAM 虽然前台线程的定位方案与 PTAM 和 ORB-SLAM 类似, 但定位效率受限于计算代价较高的 SIFT 特征.基于直接跟踪的 DTAM 和 LSD-SLAM 的定位效率主要取决于选取的图像分辨率, 因此在效率和精度之间也存在权衡, 通常分辨率越高精度越高, 但效率越低, 反之亦然。
* 场景尺度。MonoSLAM由于其$O(n^3)$的计算复杂度, 因此只适用于几百个点的小场景. MSCKF 只维护局部地图, 对场景尺度没有限制. PTAM 和 RDSLAM的场景尺度主要限制于全局 BA 和特征点匹配效率: PTAM 通常只能实时处理几千个点的中等尺度场景; RDSLAM 由于 BA 的效率更高, 而且其基于 KDTree 的匹配策略也比 PTAM 将三维点投影到当前帧搜索的策略更高效, 因此能处理的尺度更大一些, 可以实时处理几万个点的场景规模. ORB-SLAM 和 LSD-SLAM 用高效的方位图优化替代全局 BA, 适用于较大尺度的场景. DTAM 由于需要维护和渲染整个场景的三维模型, 也只适用于小场景。
* 特征缺失鲁棒性。特征缺失(比如场景缺乏丰富的纹理或者由于图像模糊导致)对几乎所有 V-SLAM 都有较大影响. 其中, 只有基于直接跟踪的 DTAM 和 LSD-SLAM 能通过利用稠密或半稠密的图像信息缓解特征依赖, 但也不能完全消除. MSCKF 属于 VI-SLAM, 即使出现短时间特征缺失的情况, 也可利用 IMU 数据来跟踪方位, 因此对临时特征缺失的鲁棒性最好。
* 快速运动和扩展效率. 能否处理相机快速运动, 一方面依赖于所采用的匹配方法在大运动情况下的鲁棒性, 另一方面也依赖于场景地图的扩展效率. MonoSLAM 采用 EKF 的方式预测特征点的位置并进行主动搜索, 这对于太大的运动会失败. PTAM 采用主要依靠运动预测的方式将关键帧上三维位置已知的特征点投影到当前帧来, 并通过基于金字塔模型的匹配来增加鲁棒性, 对快速运动的鲁棒性会比 MonoSLAM 高. 但扩展场景需要对三维位置未知的特征点进行暴力搜索特征匹配, 该步骤十分耗时, 只能放到后台进行, 因此对于快速场景扩展的效率会比 MonoSLAM 差. 与 PTAM 类似, DTAM 也先采用基于金字塔模型的整张图像对齐方法估计一个大概的相机姿态, 然后将整个模型绘制到这个视点下与当前帧图像进行进一步的对齐优化. MSCKF, RDSLAM 和 ORB- SLAM均采用对视角变化具有不变性的特征描述量 (MSCKF 和 RDSLAM 选用 SIFT 特征, ORB-SLAM 选 用 ORB 特征), 允许高效的全局特征匹配(RDSLAM 采用 KDTree, ORB-SLAM 采用词袋模型), 因此对于大运动情况具有较好的鲁棒性(RDSLAM 实际上每帧都进行全局重定位)。LSD-SLAM 假设相机平缓运动, 采用前一帧的相机姿态作初始化进行直接的图像对齐, 因此对快速运动最敏感. 类似于 PTAM, RDSLAM 和 ORB-SLAM 均在后台线程扩展场景, 但由于它们高效的特征匹配, RDSLAM 和 ORB-SLAM 扩展场景的效率优于PTAM。MSCKF因为利用了 IMU 信息, 即使匹配点还没有三角化也能扩展场景, 因此扩展场景的效率最高. LSD- SLAM 和 DTAM 由于需要恢复半稠密或稠密的深度图, 一方面计算量较大, 另一方面对于新扩展部分的深度往往需要处理至少若干帧才能收敛, 因此扩展效率最差。
* 重定位能力. 实际跟踪的时候难免遇到失败的情况, 这时需要用重定位技术从失败状态中恢复. MonoSLAM, MSCKF不支持重定位. PTAM 是通过维护关键帧的缩略图, 跟丢的时候将当前帧与已有关键帧的缩略图进行比较, 找到最匹配的关键帧恢复初始的方位后, 再把地图中的三维点投影到当前帧来进行特征匹配和方位优化来实现重定位. 该方法的缺点是, 如果当前帧跟已有的关键帧的视角不够接近, 就不容易重定位成功. RDSLAM, ORB-SLAM 和 LSD-SLAM 都是采用对视角变化具有不变性的特征描述量并结合高效的检索方法(RDSLAM 采用 KDTree, ORB-SLAM 采用词袋模型, LSD-SLAM 采用 FAB-MAP 方法)来实现重定位, 即使当前帧与关键帧有较大的视角差异也能重定位, 鲁棒性会比 PTAM 的基于缩小模糊图匹配方式更好。
* 近似纯旋转扩展鲁棒性. 在实际使用中, 相机可能会以近似纯旋转的方式转向拍摄旁边的场景内容. MonoSLAM 因为每帧同时优化三维点和相机方位, 所以对近似纯旋转有很好的鲁棒性. MSCKF 也类似, 不同的是, MSCKF 仅当跟踪轨迹结束时才三角化出三维点, 并且立即消元, 而且用了 IMU 信息, 因此对近似纯旋转最鲁棒. PTAM 和 RDSLAM 在近似纯旋转扩展场景时容易因为视差不够无法三角化新的三维点, 导致无法加入新的关键帧, 从而造成后续帧跟踪丢失, 鲁棒性较差. ORB-SLAM, DTAM 和 LSD-SLAM 对纯旋转扩展的鲁棒性很大程度取决于后台场景地图的扩展及优化的效率, 能否及时得到高质量的三维地图是后续帧能否稳定跟踪的关键. 因为 ORB-SLAM 不需要恢复稠密深度, 计算效率高, 所以在近似纯旋转扩展情况下的鲁棒性要比 DTAM 和 LSD-SLAM 更好。
* 场景变化的鲁棒性. Mono-SLAM, MSCKF, PTAM, ORB-SLAM, DTAM, LSD-SLAM 等系统都假设场景是静止不变的, 如果场景变化很大就会跟踪失败. 不过这些系统都采用了 RANSAC或鲁棒函数等策略提高鲁棒性，如果场景中动态物体上的匹配点数相对于静态匹配点数的比例不是很大, 一般能当作 Outliers 剔除掉; 但是如果场景在不断改变, 而且大部分区域都发生了变化, 那么这些系统就会失败. MSCKF 因为使用了 IMU 信息, 理论上对动态变化的鲁棒性应该会好些. RDSLAM 由于会在线地检测场景的变化, 识别出改变的三维点并更新关键帧, 而且其提出的基于时序先验的自适应 RANSAC 方法也能很好地处理误匹配点, 因此能很好地处理场景逐渐改变和动态物体干扰比较多的情况, 对场景变化的鲁棒性最高. 当然, 如果整个场景在很短的时间内完全发生改变, 那么 RDSLAM 也无法处理。
* 回路闭合能力. MonoSLAM 没有显式的回路检测步骤, 但如果系统状态一致(即协方差真实反映误差), 回路发生时仍有可能在当前帧的投影概率椭圆内搜索到回路上点的匹配点. RDSLAM 虽然没有显式的回路检测, 但是由于采用 SIFT 特征与地图里的三维点进行全局匹配, 回路发生时理论上可以检测出特征点的匹配关系并通过 BA 消除误差累积来闭合; 如果误差累积过大, 匹配点有可能会被当作 Outliers 剔除掉, 从而造成无法有效的闭合. MSCKF, PTAM 和 DTAM 均无回路闭合机制. ORB-SLAM 和 LSD-SLAM 显式检测回路构建 并优化方位图, 回路闭合能力最强。

## 3.近年研究热点与发展趋势
### A.缓解特征依赖
* V-SLAM 最大的局限在于过于依赖场景特征. 基于直接跟踪的方法通过直接对比像素颜色, 避免了对特征缺失/图像模糊非常敏感的特征提取和匹配过程, 从而很大程度上缓解了特征依赖. 然而, 稠密或半稠密的直接跟踪会引入很大的计算量, 若要运行在计算性能较低的移动设备上, 就需要将图像降采样至很小的分辨率, 那么必然会降低跟踪精度。半直接视觉测量, 只对稀疏的特征点进行直接跟踪, 能达到很高的效率.
* V-SLAM 对场景特征的依赖, 本质上是由于使用了过于底层的局部特征(点特征), 如果能利用边缘、平面等更为高层的图像信息, 也能有效地缓解特征依赖。 2008 年就有人提出使用边特征来对抗图像模糊。LSD-SLAM 实际上也是隐式地利用了图像边缘信息，大部分的半稠密区域均位于物体边缘。Concha 等提出使用面特征将图像中颜色一致的区域近似为平面称之为超 像素. Concha 等分别将超像素集成进 PTAM 和 LSD-SLAM 中, 提高原系统鲁棒性. 此外, 更为高层的空间布局对 V-SLAM 来说也是非常有价值的图像信息. 通常室内房间可以近似一个三维盒子, 通过恢复房间盒子的三维参数辅助相机跟踪或恢复高质量三维地图。

### B.稠密三维重建
* 早期的 V-SLAM 大多只能实时重建出非常稀疏的三维点云. 若要获得更为稠密的三维信息, 往往需要离线处理, 或是将实时数据传到性能强大的服务器进行计算. 微软公司于 2010 年推出 RGB-D 摄像头 Kinect, 能实时捕获彩色图像及深度图. 深度图不仅能辅助相机跟踪, 还能提供稠密的场景三维信息. Newcombe 等率先提出基于 RGB-D 的稠密 SLAM 系统 KinectFusion. KinectFusion 使用 TSDF表达三维场景, 算法在重建的三维物体上叠加一个均匀的三维网格, 每个网格体素存储其中心到最近的三维物体表面间的距离. 基于 TSDF 绘制某一视角下的深度图, 可使用 Marching Cube 算法抽取 TSDF 为 0 的三维物体表面并在该视角下进行深度绘制, 或直接进行光线投射至 TSDF 为0的深度. 为跟踪相机运动,KinectFusion绘制全局模型在上一帧视角下的深度图, 并使用 ICP 算法与当前帧深度图对齐, 求解 2 帧间相对运动. 恢复的相机方位将当前帧深度图注册至全局坐标下, 与全局模型融合, 更新 TSDF。
* KinectFusion 最明显的局限性如下: 
  * 预设的三维网格限制了场景尺度; 
  * 基于 ICP 的相机跟踪严重依赖于场景几何特征的丰富程度; 
  * 循环回路无法闭合. 
  为解决上述问题: 文献[58]提出让三维网格随相机运动, 移出三维网格的区域不参与计算, 从而突破了场景尺度的限制; 文献[59]融合了ICP, FOVIS和 RGB-D 3 种相机跟踪方法, 缓解跟踪对几何特征的依赖, 减小误差累积; 文献[62]采用 DBoW检测循环回路从而构建方位图, 并采用文献[63]提出的方法形变方位图以闭合循环回路, 重建的三维模型也相应形变; 文献[64]提出使用面元表达场景, 面元包含了三维点的位置、法向等信息, 形变直接作用于面元上而非通过方位图间接作用于三维模型, 从而获得更高的模型精度。
* 基于单目摄像头的稠密三维重建的难点在于需要实时恢复稠密的深度图, 这一过程通常都需要引入很大的计算量, 关键是如何权衡重建精度和计算效率。通常, 算法选择 2 帧具有视差的图像$(I,I')$，通过立体匹配恢复粗略的深度图。一个标准的方法是为 I 中的每个像素 x 在$(I')$中对应的极线上搜索最相似的像素$(x')$, 通常在选择局部图像块采用 NCC作为相似性度量。为加速匹配过程, 文献[67-68]对图像建造高斯金字塔, 先在低分辨率图像立体匹配, 用于限制高分辨率图像的搜索范围. MonoFusion采用 PatchMatch 技术为每个像素随机生成一个深度值, 再利用相邻像素深度值相近的特点将可靠的深度值向周围像素传递。上述局部匹配在缺少纹理或重复纹理的区域存在歧义。DTAM 和 REMODE 引入正则项对深度图进行全局优化, 缓解匹配歧义。REMODE 还采用深度滤波方法, 将每个像素的深度及 其属于 Inlier 的概率显式表达为一个混合概率模型, 不断融合每帧的深度测量更新此概率模型的参数。Kolev 等采用面元表达, 通过对深度图对应像素的深度值、法向和可见性进行一致性判断, 剔除Outliers 或更新面元参数. MonoFusion, MobileFusion
和 Schöps 等提出的基于 Project Tango 的三维重建方法都采用 TSDF来融合深度图、剔除Outliers.
* 上述方法虽然都能实时重建出稠密的三维信息, 但大多依赖于 GPU 并行计算。

### C.多传感器融合
* 基于单一传感器的定位方案不可避免地都有各自的固有局限: 仅基于图像的 V-SLAM 依赖场景纹理特征; 仅基于 IMU 的定位通常有严重的误差累积; 仅基于深度的 SLAM 依赖于场景几何特征, 且设备获取深度的精度和范围受限于设备的成本和功耗。只有将不同传感器数据融合起来, 才能互相取长补短, 达到最高的精度和鲁棒性。如今大多数移动设备都配有单目摄像头和 IMU, 有的甚至配有双目、鱼眼或深度摄像头。
* 融合多传感器数据本质上就是一个非线性优化问题, 每种类型的传感器信息作为能量函数中的一项。难点在于运动参数需要实时地恢复出来, 而变量个数会随着运行时间不断增加. 比如在 t 时刻, 待优化的相机变量为$C={C_1,C_2,...,C_t}$, 那么在 30 帧/s 的应用中, 每秒会增加 30 组待优化的运动参数, 很快便会超出计算设备的极限。
* 一个常见的简化方法为设置一个滑动窗口, 窗口中包含邻近的l帧运动参数$C_{t-l+1...C_t}$, 每次只优化这l组运动参数和相应的三维结构. 移出窗口的变量采用消元法, 得到关于剩余窗口内变量的先验约束, 加入后续的优化中。基于滤波器的 SLAM 都属于这种类型(对于标准的滤波器来说$l=1$)。很多研究工作试图提高基于滤波器的精度。MSCKF 通过加大滑动窗口大小 l 缓解由尚未精确的状态被消元而产生的误差累积。Li 等提出了改进版本 MSCKF 2.0. 通过对 MSCKF 进行可见性分析后发现, 如果线性化点不断变化, 会在原系统的不可见的艏摇(Yaw, 即绕重力方向的旋转)方向上错误地产生信息, 最终导致误差累积。于是, MSCKF 2.0 采用 First-Estimate Jacobian固定线性化点, 消除艏摇方向上的错误信息, 提高估计的精度。
* 另一类方法采用迭代优化。EKF 可以看成只进行一次迭代, 由于 SLAM 问题通常非线性, 因此迭代优化通常比 EKF 精度更高, 但显然计算量更大。为简化计算, 可采用预积分技术来避免每次迭代 IMU 数据的重复积分. 文献[79]提出一种增量优化器 iSAM2, 每次只优化当前帧影响到的局部变量，可以极大地减少计算量. 文献[13]提出在滑动窗口同时保留邻近的关键帧和非关键帧, 移出滑动窗口的信息根据对当前帧的影响选择消元或直接丢弃。

### D.其他实际问题
* 虽然目前 V-SLAM 算法在理想条件下已经能达到很高的精度和计算效率, 但在实际应用中往往无法满足理想条件. 多数单目V-SLAM系统, 尤其是基于关键帧 BA 的单目 V-SLAM 系统, 要求用户以平移的方式运动才能扩展场景。然而对于增强现实的应用来说, 用户容易以接近纯旋转的方式观看虚实融合效果, 从而导致跟踪失败; 用户在观看的时候可能不会非常平缓地运动, 实际使用过程很容易出现快速的移动和突然转动的情况, 而且这种情况下往往伴随着运动模糊, 这对目前的 V-SLAM 系统来说挑战性很大. 如何支持近似纯旋转和快速扩展场景, 提高对运动模糊的容忍能力, 对于提高系统鲁棒性、 改进增强现实应用体验尤为重要。另外, 移动设备上的摄像头往往存在滚动快门现象, 图像中每一行在不同时刻曝光, 即对应不同的相机运动。此外, 对于画面中出现的动态物体, 如果动态物体所占画面区域较大且特征丰富,传统的 RANSAC 方法很容易失败, 需要更鲁棒的误匹配点剔除机制或者利用 IMU 信息来解决歧义。

## 4.结语
* 随着各种硬件传感器的发展和普及, 目前 SLAM 技术正朝着多传感器融合的方向发展, 试图通过利用各种传感器的优势互补性来达到尽可能高的精度和鲁棒性。
