>论文标题：ViViT: A Video Vision Transformer  
发表时间：2021  
研究组织：Google Research  
本文标签：Transformer、视频目标检测、ICCV


# 速读概览：
## 1.针对什么问题？ 

## 2.采用什么方法？  

## 3.达到什么效果？  

## 4.存在什么不足？



# 论文精读
## 0.摘要
* 本文提出了基于纯transformer的视频分类模型，借鉴了此类模型在图像分类中的成功。我们的模型从输入视频中提取时空标记，然后由一系列transformer层编码。为了处理视频中遇到的长令牌序列，我们提出了我们模型的几个有效变体，它们考虑了输入的空间和时间维度。尽管基于transformer的方法被认为只在大型训练数据集可用时起作用，但我们展示了如何在训练期间有效地规范模型，并利用预训练的图像模型能够在相对较小的数据集上进行训练。我们进行了彻底的消融研究，并在包括 Kinetics 400 和 600、Epic Kitchens、Something-Something v2 和 Moments in Time 在内的多个视频分类基准上取得了最先进的结果，优于基于深度 3D 卷积网络的先前方法。

## 1.Introduction
* 自 AlexNet 以来，基于深度卷积神经网络的方法已经在许多视觉问题标准数据集中推进了最先进的技术。 同时，序列到序列建模（例如自然语言处理）中最突出的架构选择是transformer，它不使用卷积，而是基于多头自注意力 . 此操作在建模长期依赖关系方面特别有效，并允许模型参与输入序列中的所有元素。 这与对应的“感受野”的有限性且随网络深度线性增长的卷积形成鲜明对比。
* NLP 中基于注意力的模型的成功最近激发了计算机视觉中将transformer集成到 CNN 中的方法，以及一些完全替代卷积的尝试。
* 然而，直到最近使用 Vision Transformer (ViT)，基于纯transformer的架构在图像分类方面的性能才优于其卷积对应物。