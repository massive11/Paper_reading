>论文标题：Towards High Performance Video Object Detection  
发表时间：2018  
研究组织：MSRA  
本文标签：视频目标检测、CVPR


# 速读概览：
## 1.针对什么问题？ 
DFF利用连续帧之间的数据冗余减少特征计算提高了速度，但损失了精度。FGFA使用时间特征聚合提高特征质量和检测精度，但速度太慢。
## 2.采用什么方法？  
采用三种技术整合DFF和FGFA的优势。
1.利用互补属性整合DFF和FGFA的方法；2.适应性特征计算从时域拓展到空域；3.适应性关键帧选择
## 3.达到什么效果？  
77.8% mAP at speed of 15.22 fps
## 4.存在什么不足？



# 论文精读
## 0.摘要
* 图像目标检测最近几年已经取得了很大的发展。然而视频目标检测却没有收到很多关注，尽管他更具挑战性并且在实际场景中更加重要。
* 建立在之前的工作之上，本文提出了一种基于多帧端到端特征学习和跨帧运动原理的统一方法。 我们的方法使用三种新技术扩展了先前的工作，并稳步推进性能包络（速度-精度权衡），以实现高性能视频目标检测。

## 1.Introduction
* 静止图像上的目标检测发展的很快，但是直接将这些检测器应用在视频上存在很大的挑战。一是会产生无法负担的计算开销，二是视频中会出现静止图像中少见的外观退化，如运动模糊、视频散焦、rare poses等，这会导致精度下降。
* [DFF](https://github.com/massive11/Paper_reading/blob/master/%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%88%86%E5%89%B2/Deep%20Feature%20Flow%20for%20Video%20Recognition.md)、[FGFA](https://github.com/massive11/Paper_reading/blob/master/%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%88%86%E5%89%B2/Flow-Guided%20Feature%20Aggression%20for%20Video%20Object%20Detection.md)表明原则性的多帧端到端学习对于解决上述挑战是有效的。 具体来说，DFF中利用连续帧之间的数据冗余来减少大多数帧上昂贵的特征计算并提高速度。FGFA使用时间特征聚合提高特征质量和检测精度。上述工作是ImageNet Video Object Detection Challenge 2017夺魁的基础。
* 这两项工作的侧重点不同，各有各的弊端。DFF中使用稀疏特征传播保存大部分帧中的昂贵的特征计算量。这些帧的特征是从少量关键帧上廉价的传播得到的。然而，传播的特征只是近似的并且容易出错，从而损害了识别的精度。FGFA使用多帧密集特征聚合提高所有帧上的特征质量和检测精度。但是由于重复的运动估计、特征传播和聚合，它的速度很慢。
* 两部作品在性质上是相辅相成的。 它们也有相同的原理：运动估计模块内置于网络架构中，所有模块的端到端学习都是在多个帧上执行的。
* 基于这些进展和原则，本文提出了一种更快、更准确和更灵活的统一方法。 具体来说，提出了三种新技术。 
  * 首先，稀疏递归特征聚合用于保留聚合的特征质量，但也通过仅对稀疏关键帧进行操作来降低计算成本。 这种技术结合了DFF和FGFA的优点并且比两者都表现得更好。
  * 其次，引入空间自适应部分特征更新以重新计算非关键帧上传播的特征质量较差的特征。 特征质量是通过端到端训练中的新公式学习的。 该技术进一步提高了识别准确度。
  * 最后，时间自适应关键帧调度取代了之前的固定关键帧调度。它根据上面预测的特征质量来预测关键帧的使用情况。它使关键帧的使用更加高效。
* 本文所提出的技术与先前的两项工作是统一的。 综合实验表明，这三种技术稳步推进了性能（速度-精度权衡）包络，朝着高性能视频目标检测方向发展。 我们以每秒 15.22 帧的速度实现了 77.8% 的 mAP 得分。 这是最先进的水平。

## 2.From Image to Video Object Detection
* 当下大部分目标检测器的方法类似，主要由两步实现。
  * 首先通过一个全连接的backbone网络对整张输入图像I提取一系列卷积特征图F。backbone网络通常会在ImageNet分类任务上进行预训练然后再进行微调。本文称其为feature network，${N_{feat}(I) = F}$。它通常比较深并且速度慢，无法负担在全部视频帧上计算的代价。
  * 第二步是通过多分支的子网在稀疏目标proposal或者密集滑动窗口上进行区域分类和bounding box回归生成基于特征图F的检测结果y。本文称其为detection network， ${N_{det}}(F) = y}$。它是随机初始化的，并与${N_{feat}}$联合训练，通常是浅层且快速的。

### 2.1 Revisiting Two Baseline Methods on Video
#### Sparse Feature Propagation(DFF)
* 它率先引入了视频目标检测的关键帧的概念。其动机是邻近帧的相似的外观通常有相似的特征。因此没有必要在所有帧上计算特征。
* 在推理过程中，昂贵的特征网络${N_{feat}}$只应用于稀疏的关键帧上（每10th）。任何非关键帧 i 上的特征图都是通过每像素特征值扭曲和双线性插值从其前面的关键帧 k 传播的。帧间逐像素运动记录在二维运动域${M_{i\rightarrow k}}$中。从关键帧k到帧i的传播可以表示为：
$${F_{k\rightarrow i} = W(F_k, M_{i \rightarrow k}) \tag{1}}$$
其中，${W}$表示特征变形函数。然后检测网络${N_{det}}$作用在${F_{k\rightarrow i}}$上，用真实特征的近似${F_{i}}$取代了根据${N_{feat}}$计算${F_{i}}$。
* 运动域是通过光流网络估计的，${N_{flow}(I_k, I_i) = M_{i \rightarrow k}}$，它将两帧${I_k, I_i}$作为输入。包括${N_{flow}}$在内的所有模块的端到端训练，大大提高了检测精度，弥补了特征逼近带来的不准确性。与单帧检测器相比，由于${N_{flow}}$和公式(1)的计算比${N_{feat}}$中的特征提取要便宜得多，DFF方法要快得多，精度下降很小（几个mAP点）。

#### Dense Feature Aggregation（FGFA）
* 它率先引入了视频目标检测中的时序特征融合的概念。其动机是特定帧中深层的特征会被退化的特征（如运动模糊、遮挡）损坏，但是可以通过邻近帧的融合来提高。
* 在推理过程中，特征网络${N_{feat}}$在每一帧上密集的评估。对每一帧i，时间窗口${[i − r, i + r]}$（r = 2 ∼ 12 帧）内所有帧的特征图首先以与 dff 相同的方式变形到帧 i 上，形成一系列特征图${F_{k \rightarrow i}|k \in [i-r, i+r]}$。与稀疏特征传播不同，传播发生在每一帧而不仅仅是关键帧。换言之，每一帧都被视为关键帧。
* 得到帧i的聚合特征图${\overline F_i}$作为所有这些特征图的加权平均值
$${\overline{ F_i}(p) = \sum_{k \in [i-r, i+r]}W_{k\rightarrow i}(p)\cdot F_{k\rightarrow i}(p), \forall p \tag{2}}$$
其中，权重${W_{k\rightarrow i}}$自适应地计算作为传播特征图${F_{k \rightarrow i}}$和真实特征图${F_i}$之间的相似度。相反，将特征F投影到嵌入特征${F^e}$中进行相似性度量，并且该投影可以通过一个小型的全卷积网络实现。
$${W_{k\rightarrow i}(p) = exp(\frac{F^e_{k\rightarrow i}(p)\cdot F^e_i(p)}{|F^e_{k\rightarrow i}(p)|\cdot |F_i^e(p)|}), \forall p \tag{3}}$$
* 公式(2)和公式(3)都是逐位置的方式，如枚举位置 p 所示。 权重在附近帧的每个位置 p 处进行归一化，${\sum_{k \in [i-r,i+r]}W_{k\rightarrow i}(p) = 1}$
* 所有模块都是联合训练的。与单帧检测器相比，公式(2)中的聚合改善了特征，提高了检测精度（约3个mAP点），特别是对于快速运动目标（约6个mAP点）。然而，运行时间由于重复的流估计和在密集连续帧上的特征聚合慢了三倍。

## 3.High Performance Video Object Detection
* 上述两种方法的区别非常明显。DFF通过特征近似减少了特征计算量，但降低了精度。FGFA通过适应性聚合提高了特征质量，增加了计算量。它们是互补的。
* 另一方面，他们都基于两个原则：
  * 1.运动估计模块对于帧之间的有效特征级通信是必不可少的
  * 2.对所有模块的多帧进行端到端学习对于检测精度至关重要
* 基于上述原则，本文提出了一个针对高性能视频目标检测的通用框架，他提出了三个新技术。第一个利用互补属性，整合了DFA和FGFA中的方法，精度高且速度快。第二个将适应性特征计算从时域拓展到空域，使得空间适应性特征计算更加高效。第三个提出了适应性关键帧选择，进一步提高了特征计算的效率。
* 这些技术简单直观。他们自然的拓展了之前的工作。正如第 5 节中的大量实验所验证的那样，每一个都建立在前一个的基础上，并稳步推进性能（运行时精度权衡）。

### 3.1 Sparsely Recursive Feature Aggregation
* 尽管FGFA在检测速度上实现了巨大的提升，但还是非常慢。一方面，它在所有帧上密集的评估了特征网络${N_{feat}}$，然而，由于相邻帧之间的外观相似，这是不必要的。另一方面，特征聚合是在多个特征图上实施的，因此需要估计多个流域，在很大程度上降低了检测器的速度。
* 我们提出稀疏递归特征聚合，它既评估特征网络${N_{feat}}$，又仅在稀疏关键帧上应用递归特征聚合。给定两个连续的关键帧${k}$和${k'}$，${k'}$帧的聚合特征可以通过下式计算得到
$${\overline{F}_{k'} = W_{k \rightarrow k'}⊙\overline{F}_{k \rightarrow k'} + W_{k' \rightarrow k'}⊙F_{k'}}$$
其中，${\overline{F}_{k'}=W(\overline{F}_k, M_{k'\rightarrow k})}$，并且⊙代表逐元素乘法。权重在每个位置p通过${W_{k\rightarrow k'}(p) + W_{k'\rightarrow k'}(p) = 1}$相应的进行正则化。
* 这是公式(2)的递归版本，聚合只发生在稀疏的关键帧上。原则上，聚合关键帧特征${\overline{F}_{k}}$聚合了来自所有历史关键帧的丰富信息，然后传播到下一个关键帧${k'}$用于聚合原始特征${F_{k'}}$。

### 3.2 Spatially-adaptive Partial Feature Updating
* 尽管DFF通过近似真实的特征$F_i$实现了显著的速度提升，但传播的特征映射${F_{k\rightarrow i}}$容易出错，因为相邻帧之间的某些部分的外观是变化的。
* 对于非关键帧，我们希望使用特征传播的思想实现高效计算，但是公式(1)受传播质量的影响。为了量化传播的特征${F_{k \rightarrow i}}$是否是$F_i$的良好近似，引入了特征时间一致性${Q_{k \rightarrow i}}$。我们在流网络${N_{flow}}$上增加了一个兄弟分支来预测${Q_{k \rightarrow i}}$，以及运动域${M_{i \rightarrow k}}$，如
$${\{M_{i \rightarrow k}, Q_{k \rightarrow i} = N_{flow}(I_k, I_i)\} \tag{5}}$$
* 如果

### 3.3 Temporally-adaptive Key Frame Scheduling
* 高速的核心就是只在稀疏的关键帧上评估特征网络${N_{feat}}$。一个简单的关键帧调度策略以预先固定的速率选择一个关键帧，例如，每 l 帧。 更好的关键帧调度策略应该适应时域中的变化动态。 可以基于特征一致性指标${Q_{k→i}}$设计：
$${key = is\_key(Q_{k\rightarrow i}) \tag{8}}$$
* 这里我们设计了一个简单的启发式函数is_key：
$${is\_key(Q_{k \rightarrow i}) = [\frac{1}{N_p}\sum_p 1(Q_{k \rightarrow i}(p) \le \tau)] \gt \tau \tag{9}}$$
其中 1(·) 是指示函数，Np 是所有位置 p 的数量。 对于任何位置 p，${Q_{k\rightarrow i}(p) \le \tau}$表示如果要重新计算的区域${(Q_{k\rightarrow i}(p) ≤ \tau)}$大于部分${\gamma}$的所有像素，帧被标记为key。 图2展示了满足${Q_{k\rightarrow i}(p)\le \tau}$的区域随时间变化的示例。 三个橙色点是我们的is_key选择的关键帧的例子，它们的外观明显不同。 两个蓝点是非关键帧的例子，它们的外观与前一个关键帧相比确实略有变化。
* 为了探索关键帧调度的潜力和上限，我们设计了一个利用真实信息的预言机调度策略。 实验使用我们提出的方法进行，关键帧调度策略除外。 给定任意帧 i，选取帧 i 作为关键帧或非关键帧的检测结果都被计算出来，并且两个 mAP 分数也使用 ground truth 计算。 如果选择它作为关键帧会产生更高的 mAP 分数，则将帧 i 标记为关键帧。
* 这种预言机调度取得了明显更好的结果，即在 22.8 fps 的运行速度下，mAP 得分为 80.9%。 这表明了关键帧调度的重要性，并表明它是未来重要的工作方向。

### 3.4 Unified Viewpoint
* 在一个统一的观点下总结所有方法。
* 为了有效地计算特征图，使用了空间自适应部分特征更新。 虽然公式(6)只针对非关键帧定义，可以推广到所有帧。 给定一个帧 i 及其前面的关键帧 k，可以使用公式(6) ，总结为
$${\hat{F}_i = PartialUpdate(I_i, F_k, M_{i \rightarrow k}, Q_{k \rightarrow i}) \tag{10}}$$
* 对于关键帧，

### 3.5 Network Architecture
* 我们在模型中引入了不同子网络的化身。

#### Flow network
* 我们使用FlowNet的simple版本。它在Flying Chairs数据集上预训练。它应用于半分辨率图像，输出步幅为 4。由于特征网络的输出步幅为16，流场缩小一半以匹配特征图的分辨率。 添加了一个额外的随机初始化的 3x3 卷积来预测特征可传播性指标，它与 FlowNet 的最后一个卷积共享特征。

#### Feature network
* 我们使用最新的ResNet101作为特征网络。ResNet101模型是在ImageNet分类上预训练的。我们稍微修改了 ResNet-101 的性质以进行目标检测。 我们删除了结束平均池化和 fc 层，并保留了卷积层。为了提高特征分辨率，将最后一个块的有效步长从 32 更改为 16。特别是，在最后一个块的开头（ResNet-101 的“conv5” )，步幅从 2 变为 1。为了保留感受野大小，将最后一个块中卷积层（内核大小 > 1）的膨胀设置为 2。最后，随机初始化的 3×3 卷积为 应用于顶部以将特征维度减少到 1024。

#### Detection network
* 我们使用最先进的R-FCN。在 1024-d 特征图之上，应用了 RPN 子网络和 R-FCN 子网络，它们分别连接到前 512-d 和最后 512-d 特征。 RPN 中使用了 9 个锚点（3 个尺度和 3 个纵横比），每个图像产生 300 个proposal。 R-FCN 中的位置敏感分数图是 7×7 组。

## 4. Related work
### Speed/accuracy trade-offs in object detection
* 现代检测系统中速度与精度的均衡可以通过使用不同的特征网络和检测网络或改变不同的参数如图像分辨率、proposal box的数量实现。PVANET和YOLO甚至为了快速的目标检测设计了特定的特征网络。通过使用一些技巧（如批标准化、高分辨率分类器、细粒度特征和多尺度训练），YOLO9000在保持高速的同时实现了更高的精度。
* 既然我们提出的方法只考虑了如何通过利用时序信息计算更快更高质量的特征，而不是为了任意特征网络和检测网络设计的，这样的技巧对我们提出的方法也适用。

### Video object detection
* 现有的在视频中结合时间信息的目标检测方法可以分为box-level方法和feature-level方法（两者都是基于流的方法）。
* box-level的方法考虑到轨迹内的临时一致性，通常关注如何提高检测精度。T-CNN率先通过预计算的光流，将预测的bounding box传播到相邻帧。沿着每个tubelet的框将根据tubelet的分类结果重新评分。Seq-NMS从连续帧中沿附近的高置信度边界框构造序列。序列中的框被重新评分为平均置信度，其他接近该序列的框被抑制。MCMOT将后处理定义为多目标跟踪问题，最后使用跟踪置信度来重新评分检测置信度。TPN通过多帧（不超过20帧）而非单帧中的bounding box proposal生成tubelet proposal，然后每个tubelet proposal经过基于LSTM的分类器被分类成多种类别。D&T通过单个卷积神经网络同时输出检测框和基于回归的跟踪框，检测框根据跟踪框连接起来并重新评分。
* feature-level的方法通常使用光流得到邻近帧中像素到像素的对应关系。尽管feature-level方法更具原则性，并且可以进一步与box-level方法结合，但它们存在光流不准确的问题。Still ImageNet VID 2017 获胜由特征级方法DFF和FGFA组成。 我们提出的方法也是一种feature-level方法，它引入了空间自适应部分特征更新来修复由不准确的光流引起的不准确的特征传播。

## 5. Experiments
* ImageNet VID 数据集是一种流行的大规模视频对象检测基准。 模型训练和评估分别对来自训练集的 3,862 个视频片段和来自验证集的 555 个片段进行。 这些片段是完全注释的，通常帧速率为 25 或 30 fps。 有 30 个对象类别，它们是 ImageNet DET 数据集中类别的子集。

### 5.1 Evaluation under a Unified Viewpoint

### 5.2 Ablation Study

### 5.3 Comparison with State-of-the-art Methods