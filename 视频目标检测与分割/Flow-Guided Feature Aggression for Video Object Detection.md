>论文标题：Flow-Guided Feature Aggression for Video Object Detection  
发表时间：2017  
研究组织：USTC、Microsoft    
本文标签：视频目标检测、ICCV


# 速读概览：
## 1.针对什么问题？ 
针对图像的目标检测方法在处理视频目标检测问题时会出现运动模糊、视频散焦等问题而无法进行精确的检测，而现有的视频目标检测方法只是简单的在box level上进行的，通常是先在单帧中应用目标检测器，然后再专门的后处理步骤中跨时间纬度组装检测到的边界框，处理方法也是现成的运动估计方法和人工设计的边界框关联规则。这种方案实际上并不能提高检测质量，其提升在于启发式后处理，不是原则性的学习，也不存在端到端的训练。
## 2.采用什么方法？  
视频具有关于同一对象实例的丰富信息，本文通过利用视频中时间信息的连续性，通过将时间上的邻近帧的特征聚合的方式，以原则性的学习进行端到端的训练实现视频目标检测。
## 3.达到什么效果？  
算法性能与赢得2016ImageNet VID挑战的工程系统相当。
## 4.存在什么不足？
首先是网络本身速度就不够快。
其次，在我的理解上，本文的特征聚合，是采用参考帧前后各k个邻近帧进行特征聚合的，这种思路能够优化检测结果的效果是显然的。但是在实际应用上，这k帧的延迟可能就无法满足实时性的要求了。例如以70km/h的速度行驶的自动驾驶车，30帧的相机传感器进行捕捉，10帧的时间足够走6米的距离，足以造成严重的交通事故了。如果真的要以这种思路进行实时的特征聚合，可能只能通过该时刻之前的k帧进行特征聚合。


# 论文精读
## 0.摘要
* 将最新的目标检测器从图像扩展到视频是具有挑战性的。检测的精度受到视频中目标外观退化的影响，如运动模糊、视频散焦、罕见的姿势等。当前的工作尝试在box level利用时间信息，但这些方法没有经过端到端的训练。我们提出了FGFA方法，这是一种用于视频目标检测的精准的端到端的学习框架。相反，它利用了特征级别的时间连续性。它通过沿运动轨迹聚合邻近的特征提高每一帧的特征，并因此提高视频识别精度。我们的方法显著地提高了ImageNet VID上的单帧基线，特别是更具挑战性的快速移动物体。我们的方法是原则性的，并且与赢得ImageNet VID挑战2016的工程系统相当，没有花里胡哨的东西。本文提出的方法与Deep Feature Flow的组成赢得了ImageNet VID挑战2017

## 1.Introduction
* 在静止的图像上表现良好的框架应用在视频目标检测上存在很多挑战，如运动模糊、视频散焦等现象。这些框架面对快速运动的物体会显著退化
* 视频具有关于同一对象实例的丰富信息
* 现有的一些视频目标检测方法首先在单帧中应用目标检测器，然后在专门的后处理步骤中跨时间维度组装检测到的边界框。通常依赖于现成的运动估计（例如光流）和人工设计的边界框关联规则（例如对象跟踪）。 通常，此类方法操作单帧检测框的质量一般，且不会提高检测质量。性能的提升实际来自启发式后处理，而不是原则性学习。也不存在端到端的训练。以上的方法可以称为box level。
* 由于视频运动，同一目标实例的特征通常不能跨帧实现空间对齐。因此在学习过程中对运动进行建模至关重要。
* 本文提出的方法会针对每一帧提取特征得到每一帧的特征图。为了加强参考帧的特征，光流网络估计了邻近帧到参考帧之间的运动。根据流动运动，邻近帧的特征图被扭曲到参考帧。 扭曲的特征图，以及参考帧的特征图，会通过自适应加权网络进行聚合。 然后将得到的聚合特征图馈送到检测网络以在参考帧上产生检测结果。

## 2.Related work
### 图像目标检测
* 最新的通用目标检测器基本都是基于深度卷积神经网络实现的
* 本文的方法聚焦于基于视频的目标检测，不仅包含了时间信息来提高卷积特征图的质量，还能从静态图像检测器的改善中受益

### 视频目标检测
* 几乎所有现有方法都只在最后一步——边界框后处理这一步中包含时间信息。
* T-CNN 根据预先计算的光流将预测的边界框传播到相邻帧，然后通过应用来自高置信度边界框的跟踪算法生成bounding box的序列。沿着tubelets的边界框经过tubelets分类重新评分。
* Seq-NMS从连续帧中沿着邻近的高置信度边界框构建序列。 序列的框被重新评分为平均置信度，其他接近该序列的框被抑制。
* MCMOT将将后处理过程规划为多目标跟踪问题。使用一系列的手工设计的规则决定边界框是否属于跟踪的目标，并进一步修正跟踪结果。
* 上述方法都是多步实现的思路，每一个阶段的结果都依赖于上一个阶段的结果，因此很难修正之前的阶段产生的误差。

### 基于流的运动估计
* 视频中的时间信息需要原始像素或特征的对应关系，以建立连续帧之间的关系。 光流广泛应用于许多视频分析和处理中。
* 传统方法以变分方法为主，主要解决小位移。 最近的重点是大位移，组合匹配（例如，Deep-Flow、EpicFlow）已被集成到变分方法中。但这些方法都是手工制作的。
* 我们遵循深度特征流的设计，以实现跨帧的特征变形。

### 特征聚合
* 一方面，这些工作中的大多数使用循环神经网络 (RNN) 来聚合连续帧的特征。 另一方面，穷举时空卷积用于直接提取时空特征
* 这些方法中的卷积核大小可能会限制快速移动对象的建模。 为了解决这个问题，应该考虑大的内核大小，但它会大大增加参数数量，导致过拟合、计算开销大和内存消耗的问题。 相比之下，我们的方法依赖于流引导的聚合，并且可以扩展到不同类型的对象运动。

### 视觉跟踪
* 在跟踪新目标时，通过将预训练的 CNN 中的共享层与在线更新的新二元分类层相结合来创建新网络。
* 跟踪和视频目标检测任务有较大的不同，因为它在第一帧中假设一个目标的初始位置并且它不需要预测类别标签

## 3.Flow Guided Feature Aggregation
### 3.1 A baseline and motivation
* 给定输入视频帧，我们要输出每一帧上的物体边界框。基线方法是对每一帧单独的应用现有的目标检测器。
* 现代的基于CNN的目标检测器使用相似的框架。在输入图像上应用深度卷积子网得到整个图像的特征图。然后在特征图上应用浅层的特定检测子网生成输出。
* 视频帧包含同一对象实例的剧烈外观变化。因此在单帧进行检测得到的结果不够稳定，当外观表现较差时会失败。
* 这种特征传播和增强需要两个模块：1）运动引导的空间变形。 它估计帧之间的运动并相应地变形特征图。 2）特征聚合模块。 它计算出如何正确融合来自多个帧的特征。 连同特征提取和检测网络，这些是我们方法的构建块。

### 3.2 Model design
#### Flow-Guided warping
* 给定参考帧${I_i}$，邻近帧${I_j}$，流网络${F}$估计了流域
  $${M_{i\rightarrow j} = F(I_i, I_j)}$$
* 邻近帧的特征图根据流变形到参考帧，弯曲函数定义如下
  $${f_{j\rightarrow i} = W(f_j,M_{i\rightarrow j}) = W(f_j,F(I_i, I_j))}$$
  ${W(\bullet)}$是应用于特征图中每个通道所有位置的双线性变形函数，${f_{j\rightarrow i}}$表示从第 j 帧到第 i 帧变形的特征图。

#### Feature Aggregation
* 在特征变形之后，参考帧从附近的帧（包括它自己的）中积累了多个特征图。这些特征图提供了对象实例的各种信息（例如，不同的光照/视点/姿势/非刚性变形）。 为了实现聚合，我们在不同的空间位置采用不同的权重，并让所有特征通道共享相同的空间权重。
* 变形特征${f_{j\rightarrow i}}$的二维权重映射表示为${\omega _{j\rightarrow i}}$。 在参考帧${\overline f_i}$处的聚合特征获得为
  $${\overline f_i = \sum_{j=i-K}^{i+K}\omega_{j\rightarrow i}f_{j\rightarrow i}}$$
  其中K是用于聚合的邻近帧的范围（默认为10）。
* 上式类似于注意力模型的公式，其中不同的权重被分配给内存缓冲区中的特征。
* 聚合的特征${\overline f_i}$随后被送入检测子网以得到结果
  $${y_i = N_{det}(\overline f_i)}$$
* 与基线方法和之前的box level的方法相比，我们的方法在产生最终的检测结果前聚合了多帧的信息。

#### Adaptive weight
* 自适应权重表示在每个空间位置上缓冲区中的所有帧到参考帧${I_i}$的重要性${[I_{i-K},...,I_{i+K}]}$。 具体来说，在位置 p 处，如果变形特征${f_{j\rightarrow i}(p)}$靠近特征${f_i(p)}$，则为其分配更大的权重。 否则，分配较小的权重。
* 此处我们使用余弦相似度度量来衡量变形特征与从参考帧中提取的特征之间的相似度。 此外，我们不直接使用从${N_{feat}(I)}$获得的卷积特征。 相反，我们将一个微小的全卷积网络${\epsilon(\bullet)}$应用于特征${f_i}$和${f_{j\rightarrow i}}$，它将特征投影到一个新的嵌入以进行相似性度量，并被称为嵌入子网络。
* 我们通过下式估计权重
  $${\omega_{j\rightarrow i}(p) = exp(\frac{f_{j\rightarrow i}^e(p)\cdot f_i^e(p)}{|f_{j\rightarrow i}^e(p)||f_i^e(p)|})}$$
  其中${f^e = \epsilon(f)}$代表从相似性度量中嵌入特征，${\omega_{j\rightarrow i}}$在邻近帧上为一个空间位置p标准化，${\sum_{j=i-K}^{i+K}\omega_{j\rightarrow i}(p) = 1}$,权重的估计可以看作是嵌入特征之间的余弦相似度通过SoftMax运算的过程。

### 3.3 Training and Inference
#### Inference
* 给定连续帧${\{I_i\}}$  的输入视频和指定的聚合范围 K，所提出的方法顺序处理每个帧，并在附近的帧上使用滑动特征缓冲区（长度一般为 2K + 1，除了开头和结尾 K 帧）。 最初，特征网络应用于开始的 K+1 帧以初始化特征缓冲区。然后算法循环遍历所有视频帧以执行视频目标检测，并更新特征缓冲区。 依次将每一帧i作为参考帧，特征缓冲区中相邻帧的特征图相对于它进行变形，并计算它们各自的聚合权重。 然后将变形的特征聚合并馈送到检测网络进行目标检测。 在以第（i + 1）帧为参考之前，在第（i + K + 1）帧上提取特征图并将其添加到特征缓冲区。
* 至于运行时复杂度，所提出的方法与单帧基线的比率为
  $${r = 1 + \frac{(2K+1)\cdot (O(F)+O(\epsilon)+O(W))}{O(N_{feat}) + O(N_{det})}}$$
  其中${O(\cdot)}$衡量的是函数复杂度，与$N_{feat}$相比，$\epsilon$、${W}$、${N_{det}}$的复杂度可以忽略

#### Training
* 整个 FGFA 架构是完全可微的，可以进行端到端的训练。 唯一需要注意的是，特征变形模块是通过双线性插值实现的，并且关于特征图和流场也是完全可微的 。

#### Temporal dropout
* 在 SGD 训练中，聚合范围数 K 受内存限制。 我们在推理中使用大 K，但在训练中使用小 K（默认 = 2）。 这没有问题，因为自适应权重分别在训练和推理期间被正确归一化。 请注意，在训练期间，相邻帧是从与推理期间相等的大范围中随机采样的。 与 dropout技术类似，通过丢弃随机时间帧，这可以被视为时间丢失。

### 3.4 Network Architecture
* FGFA模型中引入了不同子网络的化身

#### Flow network
* 使用Flow Net。它应用于一半分辨率的图像，输出步长为 4。由于特征网络的输出步长为 16（见下文），流场缩小了一半以匹配特征图的分辨率。

#### Feature network
* 我们采用最先进的 ResNet（-50 和 -101）[14] 和 Inception-Resnet [39] 作为特征网络。 最初的 Inception-ResNet 是为图像识别而设计的。 为了解决特征错位问题并使其适合对象检测，我们使用了一个被称为“Aligned-Inception-ResNet”的修改版本，在[6]中进行了描述。
* 预训练模型在我们的 FGFA 模型中被制作成特征网络。 我们稍微修改了三个对象检测模型的性质。 我们移除了结束平均池化和 fc 层，并保留了卷积层。 为了提高特征分辨率，按照 [4, 5] 中的做法，最后一个块的有效步幅从 32 更改为 16。特别是在最后一个块的开头（“conv5”对于 ResNet 和 Aligned-Inception -ResNet)，步幅从 2 变为 1。为了保留感受野大小，最后一个块中卷积层（内核大小 > 1）的膨胀设置为 2。最后，随机初始化 3 × 3 卷积应用于顶部以将特征维度减少到 1024。

#### Embedding network
* 它具有三层：1×1×512卷积、3×3×512卷积和1×1×2048卷积。 它是随机初始化的。

#### Detection network
* 我们使用最先进的R-FCN并遵循 [49] 中的设计。 在 1024-d 特征图之上，应用了 RPN 子网络和 R-FCN 子网络，它们分别连接到前 512-d 和最后 512-d 特征。 在 RPN 中使用了 9 个锚点（3 个尺度和 3 个纵横比），每张图像产生 300 个建议。 R-FCN 中的位置敏感分数图是 7 × 7 组。

## 4. Experients

## 5. Conclusion and Future work
* 这项工作提出了一个用于视频目标检测的，精确的、端到端的、原则性的学习框架。由于我们的方法集中于提高特征质量，它将与现有的box level框架互补，以提高视频帧的准确性。 有几个重要方面有待进一步探索。我们的方法有点慢，而且它可能会被更轻量级的流网络加速。 在快速物体运动方面还有很大的改进空间。 更多的注释数据（例如，YouTube-BoundingBoxes）和精确的流量估计可能有利于改进。 我们的方法可以在聚合中进一步利用更好的自适应记忆方案，而不是使用的注意力模型。 我们相信这些开放性问题将激发更多未来的工作。