>论文标题：Target-driving Visual Navigation in Indoor Scenes using Deep Reinforment Learning  
发表时间：2017  
研究组织：Stanford University, Li FeiFei's team  
本文标签：室内导航、DRL、视觉导航、ICRA


# 四问：
## 1.针对什么问题？ 
在仅有视觉输入的条件下进行室内导航寻找目标物体。 
## 2.采用什么方法？  
将视觉任务目标作为输入，学习策略融合目标和当前状态，使用3D仿真环境进行训练。
## 3.达到什么效果？  
能够避免传统的强化学习模式中每次确定新的目标需要重训练的问题，提高了模型的通用性并减少了计算量。模型是在仿真环境中训练得到的，经过简单的转换工具即可移植到真实的机器人上使用。
## 4.存在什么不足？
仿真环境有限，进行更多的训练需要增加模型。


# 论文精读
## 0.摘要
### 讨论的问题：
    解决深度强化学习现阶段存在的两个问题，对新目标的通用性不足（泛化能力？）和数据计算的低效性。
### 主要贡献：
    提出了速度更快、通用性更强、能从仿真环境训练并向真实环境泛化、能端到端的训练的方法。

## 1.介绍
* 机器人与外界交互的基础是理解某种行为及其对环境产生的影响。
* 本文着眼于解决在仅有视觉输入的条件下，在三维空间中导航找到一个给定的目标
* 传统的深度强化学习是通过学习仅依赖于当前状态的策略，目标则暗含在模型参数中。因此，每确立一个新目标，都必须修改模型参数，训练过程中的计算量非常大。
* 本文提出了一种目标驱动式的模型，将视觉目标作为输入，避免重复训练的问题。模型学习的策略联合了目标和当前状态，因此不需要为了新的目标而重新训练模型。这带来了一个非常大的好处，多轮训练可以共享信息，不同的场景也能提供场景中的通用特性。
* 使用常规的图像数据集收集技术获取大规模的真实环境下的行动的相互影响的数据十分困难。本文开发了一种具有高质量3D场景的仿真框架。

## 2.相关工作
### 关于视觉导航
* 基于地图的导航方法需要当前环境的全局地图来进行决策导航。  
  针对这类方法，本文的优势在于不需要当前环境的先验地图。
* 另一类是即时重建地图并用其导航，或经过人类指导的训练阶段来构建地图。  
  对比来看，本文的方法不需要环境地图，因为它对环境地标没有假设，也无需进行人工训练。
* 无地图导航方法也非常常见。这些方法集中在给定输入图片的避障上。  
  本文的方法是无地图的，但包含对环境的隐含知识。
**本文的方法并非基于特征匹配和三维重建，且不需要监督训练来识别独特的地标。**
### 关于强化学习
* 文献【23】提出了一种用于四足机器人运动的策略梯度RL算法(policy gradient RL)
* 文献【24】提出了一种运动原语学习的策略梯度算法
* 文献【25】提出了一种使用单目相机的基于强化学习的障碍检测算法
* 文献【26】将强化学习用于自主直升机飞行
* 文献【27】使用强化学习实现自动化数据收集来建图
* 文献【28】提出了一种用于大规模设置的基于内核的强化学习算法
* 文献【29】使用强化学习在ATARI游戏中进行决策  
 **相比于上述方案，本文使用深度强化学习来处理高维感官输入。**
### 关于深度学习与强化学习结合
* 文献【2】提出了深度Q网络玩ATARI游戏
* 文献【3】提出了一种DRL方法，其中深度网络的参数由环境中代理的多个异步副本更新

近来，物理引擎已被用于从图像中学习真实世界场景的动态。**本文展示了从仿真环境中训练的模型能够推广到现实世界的场景。**

## 3.AI2-THOR 框架
### 框架的作用
    在3D环境中执行操作并观察结果
### 框架的功能
    即插即用，可以轻松应用于不同类型的环境；
    具有当前场景的物体细节模型，以便正确表示运动和物体之间的交互
### 模型实现
    通过物理引擎Unity3D与深度学习框架Tensorflow集成设计的。
    大体思路是将物理引擎渲染出来的图像传输到深度学习框架，深度学习框架根据基于视觉输入的控制端发送命令并发回物理引擎中的处理端。
### 模型的优势
    1.物理引擎和深度学习框架可以直接交互，从环境中得到的反馈可以立即用于线上决策的制定；
    2.模型的构造遵循真实世界的外观，对于推广到现实世界非常重要。


## Target-driven navigation model
### 模型的目的
    寻找从当前位置移动到由RGB图像定义的目标位置的最短动作序列
### 输入与输出
    输入是当前观察到的RGB图像和目标所在地的RGB图像，输出是3D空间中的运动。模型学习的是2D图像到3D空间中的行动的映射
### 强化学习要素
    强化学习要素包括行动空间、观察和目标、奖励设计。
* 行动空间：本文使用控制级别的行动进行训练。对于导航任务，只考虑前后左右四个方向的移动，且限制每一步的长度为0.5m，转角为90度，将场景空间转换成网格表示。
* 观察和结果：观察和结果都是由事件RGB照相机从第一视角采集到的图片。
* 奖励设计：只针对任务完成情况提供一个达到目标的奖励。为了刺激更短的路径，增加一个小小的时间惩罚作为即时奖励。
### 模型设计
* 推理当前位置和目标之间的空间排列的方法是将它们投影到相同的嵌入空间中去，保留他们的几何关系。使用两个权重共享的孪生层流将当前状态和目标转换到相同的嵌入空间。 
* 所有场景中的目标共享相同的通用连体层，场景中的所有目标共享相同的场景特定层，这使得模型能够更好地跨目标和跨场景泛化。
 

## 结论
* 本文的工作是强化学习走向更加现实的应用的一步尝试，解决了深度强化学习在向现实场景中应用存在的通用性问题，并提高了数据处理效率。此外，提出了AI2-THOR框架实现低成本、高效率的行动和交互数据采集。
* 未来的工作：提高框架中的3D场景数量