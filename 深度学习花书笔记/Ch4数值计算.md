>书名：深度学习（花书）    
时间：2021年11月3日      
本文标签：深度学习、数值计算

# 4 数值计算
## 4.1 上溢和下溢
* 下溢：一种舍入误差，当接近零的数被四舍五入为零时发生下溢
* 上溢：一种数值错误，当大量级的数被近似为$\infty$或$-\infty$时发生上溢

## 4.2 病态条件
* 条件数指的是函数相对于输入的微小变化而变化的快慢程度

## 4.3 基于梯度的优化方法
* 优化是指改变x以最小化或最大化某个函数f(x)的任务，通常以最小化指代大多数最优化问题
* 要最小化或最大化的函数称为目标函数或准则，对其进行最小化时，也称为代价函数、损失函数或误差函数
* <font color=red>通常使用一个上标$*$表示最小化或最大化函数的x值，如记${x^* = arg min f(x)}$</font>
* 梯度下降：将x往导数的反方向移动一小步来减少f(x)称为梯度下降
* 导数为0的点称为临界点或驻点，包括极小点、极大点和鞍点
* 鞍点；既不是最大点也不是最小点的临界点
* 在多维情况下，临界点是梯度中所有元素都为0的点
* 偏导数衡量点x处只有$x_i$增加时$f(x)$如何变化，梯度是相对一个向量求导的导数，f的梯度是包含所有偏导数的向量，记为${\nabla_xf(x)}$。梯度的第i个元素是f关于$x_i$的偏导数
* 在u（单位向量）方向的方向导数是函数f在u方向的斜率。换言之，方向导数是函数${f(x+\alpha u)}$关于$\alpha$的导数（在$\alpha = 0$时取得）。使用链式法则，可以看到当$\alpha = 0$时，${\frac{\partial}{\partial \alpha}f(x+\alpha u) = u^{\top}}\nabla_xf(x)$。
* 为了最小化f，希望找到使f下降的最快的方向。计算方向导数：
  $${min_{u,u^{\top}u=1}u^{\top}\nabla_xf(x) = min_{u,u^{\top}u=1}\|u\|_2 \|\nabla_xf(x)\|_2 cos \theta = min_u cos\theta}$$
  u与梯度方向相反时取得最小。因此在负梯度方向（指向下坡）上移动可以减小f，这就是<font color=red>最速下降法（梯度下降法）</font>
* 最速下降建议新的点为
  $${x' = x - \epsilon \nabla_xf(x)}$$
  其中${\epsilon}$称为学习率，是一个确定步长大小的量
* 递增带有离散参数的目标函数称为爬山

### 4.3.1 梯度之上：Jacobian和Hessian矩阵
* 有时需要计算输入和输出都为向量的函数的所有偏导数，包含所有这样的偏导数的矩阵称为Jacobian矩阵。
* 假设有一个函数$f:{R^m \rightarrow R^n}$，其Jacobian矩阵${J \in R^{n \times m}}$定义为
  $${J_{i,j} = \frac{\partial}{\partial x_j}f(x)_i}$$
* 导数的导数称为二阶导数
* 当函数具有多维输入时，二阶导数也有很多，这些导数合并成一个矩阵，称为Hessian矩阵,${H(f)(x)}$定义为
  $${H(f)(x)_{i,j} = \frac{\partial^2}{\partial x_i \partial x_j}f(x)}$$
  Hessian等价于梯度的Jacobian矩阵
* 微分算子在任何二阶偏导连续的点处可交换，即Hessian矩阵在这些点上是对称的。在深度学习背景之下，几乎处处是对称的。由于其是实对称的，可将其分解为一组特征值和一组特征向量的正交基
* 在特定方向d上的二阶导数可以写成${d^\top Hd}$。当d是H的一个特征向量时，这个方向的二阶导数就是对应的特征值
* 可以通过（方向）二阶导数预期一个梯度下降步骤能表现的多好。在当前点${x^{(0)}}$处做函数${f(x)}$的近似二阶泰勒级数
  $${f(x) \approx f(x^{(0)}) + (x - x^{(0)})^{\top}g + \frac{1}{2}(x - x^{(0)})^{\top}H(x-x^{(0)})}$$
  其中，g是梯度，H是${x^{(0)}}$点的Hessian。若使用学习率${\epsilon}$，新的点x将会是${x^{(0)}-\epsilon g}$。代入上式得
  $${f(x^{(0)} - \epsilon g )\approx f(x^{(0)}) - \epsilon g^{\top}g + \frac{1}{2} \epsilon^2g^{\top}Hg}$$
* 二阶导数还可用于确定一个临界点是否是局部极大点、局部极小点或鞍点。
* 当Hessian矩阵是正定的（所有特征值都是正的），则该临界点是局部极小点（因为方向二阶导数在任意方向都是正的）
* 当Hessian矩阵是负定的（所有特征值都是负的），则该临界点是局部极大点
* 多维情况下，单个点处每个方向的二阶导数是不同的，Hessian的条件数衡量这些二阶导数的变化范围。当其条件数很差时，梯度下降法也会表现得很差。可能在一个方向上导数增加的很快但是在另一个方向上导数增加的很慢，梯度下降法不知道应该优先探索导数长期为负的方向。这时可以借助Hessian矩阵的信息解决这个问题。其中最简单的方法是牛顿法
* 牛顿法基于一个二阶泰勒展开来近似${x^{(0)}}$附近的${f(x)}$
  $${f(x) \approx f(x^{(0)}) + (x - x^{(0)})^{\top}\nabla_xf(x^{(0)}) + \frac{1}{2}(x - x^{(0)})^{\top}H(f)(x^{(0)})(x-x^{(0)})}$$
  通过计算，可以得到这个函数的临界点：
  $${x^* = x^{(0)} - H(f)(x^{(0)})^{-1}\nabla_Xf(x^{(0)})}$$
* 如果f是一个正定二次函数，牛顿法应用一次上式即可直接跳到最小值，否则需要多次迭代，但仍会比梯度下降更快。只有当附近的临界点是最小点时（Hessian的所有特征值都是正的），牛顿法才适用。
* 仅使用梯度信息的优化算法称为一阶优化算法，如梯度下降。使用Hessian矩阵的优化算法称为二阶最优化算法，如牛顿法。

## 4.4 约束优化
* 有时需要在x的某些集合$S$中找$f(x)$的最大值或最小值。这称为约束优化。集合$S$中的点x称为可行点。
* 可以通过强加一个范数约束找到在某种意义上小的解