>书名：深度学习（花书）    
时间：2021年11月4日      
本文标签：深度学习、前馈网络

# 6 深度前馈网络
* 深度前馈网络也称前馈神经网络或多层感知机
* 前馈网络的目的是近似某个函数${f^*}$
* 前馈网络定义了一个映射${y=f(x;\theta)}$，并学习参数$\theta$的值，使其能够得到最佳的函数近似。
* 这种模型被称为前向的，是因为信息流过x的函数，流经用于定义f的中间计算过程，最终到达输出y。在模型的输出和模型本身之间没有反馈。
* 前馈神经网络被称为网络是因为他们通常用许多不同的函数复合在一起来表示。
* 如用三个函数链在一起
  $${f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))}$$
  链的全长被称为模型的深度。
* 训练数据直接指明了输出层在每一点x上必须做什么；它必须产生一个接近y的值。但是未直接指明其他层应该怎么做，也没有说每个单独的层应该做什么。因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为隐藏层。
* 网络中的每个隐藏层通常都是向量值的，这些隐藏层的维数决定了模型的宽度。
* 线性模型的能力被局限在线性函数里了。为扩展线性模型，可以将线性模型用于一个变换后的输入${\phi(x)}$上，这里$\phi$是一个非线性变换，它提供了x的一个新的表示。剩下的问题就是如何选择映射$\phi$。深度学习的策略是去学习$\phi$
* 假设有一个模型
  $${y = f(x;\theta,\omega) = \phi(x;\theta)^{\top}\omega}$$
  现在有两种参数：用于从一大类函数中学习$\phi$的参数$\theta$，以及用于将$\phi(x)$映射到所需的输出的参数$\omega$.$\phi(x)$定义了一个隐藏层。
* 实质上是通过学习特征来改善模型的一般化原则。

## 6.1 实例：学习XOR
* 对于异或问题，使用全部四个点${X = \{ [0,0]^{\top},[0,1]^{\top},[1,0]^{\top},[1,1]^{\top}  \}}$训练网络，唯一的挑战是拟合训练集。这个问题可以视作回归问题，使用均方误差损失函数(实际上，对二进制数据建模时，MSE通常并不是一个合适的代价函数)
* 目标函数：
  $${y = f^*(x)}$$
* 模型给出的函数
  $${y = f(x;\theta)}$$
  学习算法将不断调整参数$\theta$来使得$f$尽可能接近$f^*$。
* 评估整个训练集上表现的MSE代价函数为
  $${J(\theta) = \frac{1}{4} \sum_{x\in X}(f^*(x) - f(x;\theta))^2  }$$
* 现在需要选择模型${y = f(x;\theta)}$的形式。假设选择线性模型，$\theta$包含$\omega$和$b$，那么模型被定义为
  $${f(x;\omega,b) = x^{\top}\omega + b}$$
  可以使用正规方程关于$\omega$和$b$最小化${J(\theta)}$得到一个闭式解
* 解正规方程得到$${\omega=0,b=\frac{1}{2}}$$
* 此处引入简单的前馈神经网络，有一层隐藏层且隐藏层中包含两个单元，网络现在包含衔接在一起的两个函数：
  $${h = f^{(1)} (x;W,c)}$$
  $${y = f^{(2)} (h;\omega,b)}$$
* 激活函数g通常选择对每个元素分别起作用的函数，有${h_i = g(x^{\top}W_{:,i}+c_i)}$。现代神经网络中默认的推荐是由激活函数${g(z) = max\{0,z\}}$定义的整流线型单元或者称为RELU。
* 整个网络是
  $${f(x;W,c,\omega,b) = \omega^{\top}max\{ 0,W^Tx+c \} + b}$$
* 令
  $${W=\begin{bmatrix}1 & 1 \\ 1 & 14\\ \end{bmatrix}}$$
  $${c=\begin{bmatrix}0\\ -1\\ \end{bmatrix}}$$
  $${c=\begin{bmatrix}1\\ -2\\ \end{bmatrix}}$$
  以及b=0
* 最终计算得到的结果说明模型误差为0 

## 6.2 基于梯度的学习
* 凸优化从任何一种初始参数出发都会收敛，用于非凸损失函数的随机梯度下降没有这种收敛性保证，并且对参数的初始值很敏感。对于前馈神经网络，将所有的权重值初始化为小随机数是很重要的。偏置可以初始化为零或小的正值。
* 为了使用基于梯度的学习方法，必须使用一个代价函数，并且必须选择如何表示模型的输出。

### 6.2.1 代价函数

#### 6.2.1.1 使用最大似然学习条件分布
* 大多现代的神经网络使用最大似然来训练，这意味着函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。这个代价函数表示为
  $${J(\theta) =-E_{X,Y\sim \hat p_{data}}log p_{model}(y|x)}$$
* 代价函数的梯度必须足够的大和具有足够的预测性，来为学习算法提供好的指引。负的对数似然能够帮助解决产生隐藏单元或输出单元的输出的激活函数变得饱和的问题（饱和的函数梯度很小）。负对数似然代价函数消除了某些输出单元中的指数效果。

#### 6.2.1.2 学习条件统计量
* 均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度。

### 6.2.2 输出单元
* 代价函数的选择与输出单元的选择紧密相关，大多数时间简单地使用数据分布和模型分布间的交叉熵。

#### 6.2.2.1 用于高斯输出分布的线性单元
* 一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性。往往直接被称为线性单元。
* 给定特征h，线性输出单元层产生一个向量
  $${\hat y = W^{\top}h + b}$$

#### 6.2.2.2 用于Bernoulli输出分布的sigmoid单元
* 二分类问题可以归结为预测二值型变量y的值，此时最大似然的方法是定义y在x条件下的Bernoulli分布。
* Bernoulli分布仅需单个参数来定义，神经网络只需预测${P(y=1|x)}$即可。通过阈值来限制它成为一个有效的概率。
  $${P(y=1|x) = max\{ 0,min\{ 1,\omega^{\top}h + b \} \}}$$
* 当${\omega^{\top}h + b}$处于单位区间外的时候，模型的输出对其参数的的梯度都将为0.最好是使用一种新的方法来保证无论何时模型给出了错误的答案时，总能有一个较大的梯度。这种方法是基于使用sigmoid输出单元结合最大似然来实现的。
* sigmoid输出单元定义为
  $${\hat y = \sigma(\omega^{\top}h + b)}$$
  此处${\sigma}$是logistic sigmoid函数
* 最大似然的代价函数中的log抵消了sigmoid中的exp，因此最大似然总是训练sigmoid输出单元的优选方法

#### 6.2.2.3 用于Multinoulli输出分布的softmax单元
* softmax函数最常作为分类器的输出
* 首先，线性层预测了未归一化的对数概率
  $${z = W^{\top}h+b}$$
  其中，${z_i = log \hat P(y=i|x)}$
* softmax函数可以对z指数化和归一化来获得需要的${\hat y}$，最终softmax的形式为
  $${softmax(z)_i = \frac{exp(z_i)}{\sum_j exp(z_j)}}$$
* 当使用最大化对数似然训练softmax时，使用指数函数工作的非常好

## 6.3 隐藏单元
* 整流线性单元是隐藏单元极好的默认选择。
  $${g(z) = max\{0,z\}}$$
  在${z=0}$处不可微
* 大多数的隐藏单元都可以描述为接受输入向量x，计算仿射变换${z = W^{\top}h+b}$，然后使用一个逐元素的非线性函数${g(z)}$。大多数隐藏单元的区别仅仅在于激活函数${g(z)}$的形式。

### 6.3.1 整流线性单元及其扩展
* 整流线性单元的一个缺陷是他们不能通过基于梯度的的方法学习那些使它们激活为0的样本。整流线性单元的各种扩展保证了他们能在各个位置都接收到梯度。

## 6.4 架构设计
* 架构是指网络的整体结构：他应该具有多少单元，这些单元应该如何连接
* 大多数神经网络被组织成称为层的单元组。大多数神经网络架构将这些层布置成链式结构，其中每一层都是前一层的函数。

### 6.4.1 万能近似性质和深度
* 万能近似定理表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何“挤压”性质的激活函数的隐藏层，只要给予网络足够数量的隐藏单元，他可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。

## 6.5 反向传播和其他的微分算法
* 前馈神经网络接收输入x并产生输出${\hat y}$，信息沿网络向前流动。输入x提供初始信息，然后传播到每一层的隐藏单元，最终产生输出${\hat y}$，这称之为前向传播
* 在训练过程中，前向传播可以持续向前直到它产生一个标量代价函数${J(\theta)}$
* 反向传播算法，简称为backprop，允许来自代价函数的信息通过网络向后流动，以便计算梯度。

## 6.6 历史小记
* 前馈网络可以被视为一种高效的非线性函数近似器，它以使用梯度下降来最小化函数近似误差为基础。