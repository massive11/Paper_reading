>书名：深度学习（花书）    
时间：2021年11月3日      
本文标签：深度学习、机器学习

# 5 机器学习基础
## 5.1 学习算法
* 机器学习是一种能够从数据中学习的算法。
* 对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。

## 5.2 容量、过拟合和欠拟合
* 模型的容量是指其拟合各种函数的能力，容量低可能很难拟合训练集，容量高可能会过拟合
* 控制训练算法容量的方法是选择假设空间，即学习算法可以选择为解决方案的函数集
* 事实上，学习算法不会找到最优函数，而仅是找到一个可以大大降低训练误差的函数

### 5.2.2 正则化
* 没有免费的午餐定理暗示我们必须在特定任务上设计性能良好的机器学习算法，可以通过建立一组学习算法的偏好达到这个要求。
* 可以通过两种方式控制算法的性能：
  * 允许使用的函数种类
  * 函数的数量
* 在假设空间中，可能有两个函数都符合条件，但是更偏好其中一个，只有当非偏好函数比偏好函数在训练数据上表现的好得多的时候才会考虑非偏好函数。
* 可以通过加入权重衰减修改线性回归的训练标准。
* 更一般地，正则化一个学习函数${f(x;\theta)}$的模型，可以给代价函数添加被称为正则化项的惩罚
* 表示对函数的偏好是比增减假设空间更一般地控制模型容量的方法
* 正则化是指修改学习算法，使其降低泛化误差而非训练误差

## 5.3 超参数和验证集
* 超参数可以设置来控制算法行为，其值不是通过学习算法本身能学出来的
* 将训练数据分成两个不相干的子集，其中一个用于学习参数，另一个作为验证集，用于估计训练中或训练后的泛化误差，更新超参数

### 5.3.1 交叉验证
* k折交叉验证过程：将数据集分成k个不重合的子集，测试误差可以估计为k次计算后的平均测试误差

## 5.4 估计、偏差和方差

### 5.4.2 偏差
* 估计的偏差被定义为：
  $${bias(\hat \theta_m) = E(\hat \theta_m) - \theta}$$
* 如果${bias(\hat \theta_m)=0}$，那么估计量${\hat \theta_m}$被称为是无偏的，这意味着${E(\hat \theta_m) = \theta}$
* 如果${lim_{m \rightarrow \infty}bias(\hat \theta_m)=0}$，那么估计量${\hat \theta_m}$被称为是渐进无偏的，这意味着${lim_{m \rightarrow \infty}E(\hat \theta_m) = \theta}$

### 5.4.3 方差和标准差
* 样本方差的平方根和方差无偏估计的平方根都不是标准差的无偏估计。这两种计算方法都倾向于低估真实的标准差，但仍用于实际中
* 中心极限定理告诉我们均值会接近一个高斯分布，可以用标准差计算出真实期望落在选定区间的概率。例如，以均值为中心的95%置信区间是
  $${(\hat \mu_m - 1.96SE(\hat \mu_m), \hat \mu_m + 1.96SE(\hat \mu_m))}$$
  以上区间是基于均值${\hat \mu_m}$和方差${SE(\hat \mu_m)^2}$的高斯分布。<font color=red>在机器学习实验中，通常说算法A比算法B好，是指算法A的误差的95%置信区间的上界小于算法B的误差的置信区间的下界。</font>
* 注意样本方差的推导

### 5.4.4 权衡偏差和方差以最小化均方误差
* 偏差和方差度量着估计量的两个不同误差来源，偏差度量着偏离真实函数或参数的误差期望，方差度量着数据上任意特定采样可能导致的估计期望的偏差
* 在偏差更大的估计和一个方差更大的估计进行权衡时，可以使用交叉验证进行判断。
* 也可以比较这些估计的均方误差
  $${MSE = E[(\hat \theta_m - \theta)^2] = Bias(\hat \theta_m)^2 + Var(\hat \theta_m)}$$
  其中$\theta_m$是训练样本的均值，$\theta$是真实均值
  MSE度量着估计和真实参数$\theta$之间平方误差的总体期望偏差
* 用MSE度量泛化误差时，增加容量会增加方差，降低偏差

## 5.5 最大似然估计
* 最大似然估计是评估某些函数是否是好的估计的最常用准则，是机器学习中的首选估计方法 

## 5.6 贝叶斯统计
* 上述内容属于频率派统计方法，基于估计单一值${\theta}$做所有的预测。频率派的视角是真实参数是未知的定值，点估计是考虑数据集上函数的随机变量。
* 贝叶斯统计的视角完全不同。贝叶斯统计是在做预测时会考虑所有可能的$\theta$。贝叶斯统计用概率反应知识状态的确定性程度。数据集是直接能观察到的，因此不是随机的。真实参数是未知的或不确定的，可以表示成随机变量。
* 在观察到数据前，将$\theta$的已知知识表示成先验概率分布
* 假设我们有一组数据${\{ x^{(1)},..., x^{(m)} \}}$，通过贝叶斯规则结合数据似然${p(x^{(1)},...,x^{(m)}|\theta)}$和先验，可以恢复数据对我们关于$\theta$信念的影响：
  $${p( \theta | x^{(1), ..., x^{(m)}}) = \frac{ p(x^{(1)},...,x^{(m)}|\theta) p(\theta)}{p(x^{(1)},...,x^{(m)})}}$$
* 贝叶斯估计对下一个数据样本的预测来源于每个具有正概率密度的$\theta$的值，贡献由后验密度本身加权，而最大似然预测时使用${\theta}$的点估计
* 对贝叶斯方法的批判认为先验分布是人为主观判断影响预测的来源
* 当训练数据很有限时，贝叶斯方法通常泛化的很好；但是当训练样本数目很大时，会有很大的计算代价

## 5.7 监督学习方法
* 