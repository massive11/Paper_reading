>书名：深度学习（花书）    
时间：2021年11月1日      
本文标签：深度学习、概率论、信息论

# 3 概率与信息论
## 3.3 概率分布
* 概率分布用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性大小。

### 3.3.1 离散型变量和概率质量函数
* 离散型变量的概率分布用概率质量函数（PMF）描述。
* 多个变量的概率分布称为联合概率分布

### 3.3.2 连续性变量和概率密度函数
* 连续性变量的概率分布用概率密度函数（PDF）描述。
* 对概率密度函数求积分可以获得点集的真实概率质量

## 3.4 边缘概率
* 从一组变量的联合概率分布中得到自己上的概率分布称为边缘概率分布
* 离散型随机变量的求和法则
  $${\forall x \in X,P(X = x) = \sum _yP(X=x,Y=y)}$$
* 连续型随机变量的积分法
  $${p(x) = \int p(x,y)dy}$$

## 3.5 条件概率
* 条件概率公式
  $${P(Y=y|X=x) = \frac{P(Y=y,X=x)}{P(X=x)}}$$

## 3.6 条件概率的链式法则
* 任何多维随机变量的联合分布都可以分解成只有一个变量的条件概率相乘的形式
  $${P(X^{(1)},...,X^{(n)}) = P(X^{(1)}) \prod_{i=2}^n P(X^{(i)}|X^{(1)}, ..., X^{(i-1)})}$$
  <font color=red>这个规则称为概率的链式法则或者乘法法则</font>

## 3.7 独立性和条件独立性
* 满足下列式子的两个随机变量是相互独立的，记为${x\bot y}$
  $${\forall x \in X,y \in Y,p(X=x,Y=y) = p(X=x)p(Y=y)}$$
* 若关于x和y的条件概率分布对于z的每一个值都可以写成乘积的形式，那么x和y在给定随机变量z时是条件独立的，记为${x\bot y|z}$
  $${\forall x \in X,y \in Y, z \in Z,p(X=x,Y=y|Z=z) = p(X=x|z=z)P(Y=y|Z=z)}$$

## 3.8 期望、方差和协方差
* 离散型随机变量的均值
  $${E_{x \sim p}[f(x)] = \sum_xP(x)f(x)}$$
* 连续型随机变量的均值
  $${E_{x \sim p}[f(x)] = \int p(x)f(x)dx}$$
* 期望是线性的
* 方差衡量的是随机变量的函数值会呈现多大的差异
  $${Var(f(x)) = E[(f(x)-E(f(x)))^2]}$$
* 当方差较小时，$f(x)$的值形成的簇比较接近他们的期望值。方差的平方根被称为标准差。
* 协方差给出了两个变量线性相关性的强度以及这些变量的尺度
  $${Cov(f(x),g(y)) = E[(f(x) - E[f(x)])(g(y) - E[g(y)])]}$$
* 协方差的绝对值如果很大，则变量值变化很大，距离各自的均值较远。
* 如果协方差是正的，则两个变量都倾向于同时取相对较大的值。
* 如果协方差是负的，则一个变量取相对较大的值的同时，另一个变量取相对较小的值。
* （两个变量都取相对较小的值的情况呢？应该也是正的吧）
* 相关系数：将每个变量的贡献归一化，只衡量变量的相关性而不受各个变量尺度大小的影响。
* 如果两个变量相互独立，则协方差为0；若协方差为0，他们之间一定没有线性关系。
* 随机变量$x\in R^n$的协方差矩阵是一个$n \times n$的矩阵，满足
  $${Cov(X)_{i,j} = Cov(X_i,X_j)}$$
* 协方差矩阵的对角元是方差：
  $${Cov(X_i,X_i) = Var(X_i)}$$

## 3.9 常用概率分布
### 3.9.1 Bernoulli分布
* Bernoulli分布是单个二值随机变量的分布
  $${P(X=x) = \phi^x(1-\phi)^{1-x}, \phi \in [0,1]}$$
* 性质如下
  $${E_X[X] = \phi}$$
  $${Var_X(X) = \phi(1-\phi)}$$

### 3.9.2 Multinoulli分布（不太理解）
* Multinoulli分布或者范畴分布是指在具有k个不同状态的单个离散型随机变量上的分布，其中k是一个有限值
* Multinoulli分布由向量${p \in [0,1]^{k-1}}$参数化，其中每一个向量$p_i$表示第i个状态的概率。最后的第k个状态可通过${1 - 1^{\top}p}$给出。
* 常用来表示对象分类的分布，通常不需要计算其均值和方差

### 3.9.3 高斯分布
* 高斯分布也称正态分布
  $${\Nu(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi \sigma^2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}}$$
* 更高效的参数化方法是使用参数${\beta \in (0, \infty)}$来控制分布的精度
  $${\Nu(x;\mu,\beta^{-1}) = \sqrt{\frac{\beta}{2\pi}}e^{-\frac{\beta}{2}(x-\mu)^2}}$$
* 正态分布是对模型加入的先验知识量最少的分布
* 正态分布推广到${R^n}$空间，称为多维正态分布，其参数是一个正定对称矩阵$\Sigma$
  $${\Nu(x;\mu,\Sigma) = \sqrt{\frac{1}{(2\pi)^n det(\Sigma)}}e^{-\frac{1}{2}(x-\mu)^{\top}\Sigma^{-1}(x-\mu)}}$$
  参数${\Sigma}$给出了分布的协方差矩阵

### 3.9.4 指数分布和Laplace分布
* 指数分布（在x=0处取得边界点的分布）
  $${p(x;\lambda) = \lambda 1_{x \ge 0}e^{-\lambda x}}$$
* Laplace分布允许我们在任意一点${\lambda}$处设置概率质量的峰值：
  $${Laplace(x;\mu,\nu) = \frac{1}{2\nu}e^{-\frac{\vert {x-\mu}\vert}{\nu}}}$$

### 3.9.5 Dirac分布和经验分布
* Dirac delta函数定位概率密度函数（概率分布中的所有质量都集中在一个点上）
  $${p(x) = \delta (x-\mu)}$$
  除了0以外，所有的点的值都为0，但积分为1
* Dirac delta函数属于广义函数，是根据积分性质定义的数学对象
* Dirac delta函数经常作为经验分布的一个组成部分出现
  $${\hat p(x) = \frac{1}{m}\sum_{i=1}^m\delta(x-x^{(i)})}$$

### 3.9.6 混合的分布
* 常见的混合模型有高斯混合模型，它的组件是高斯分布，每个组件都有各自的参数。高斯混合模型的参数指明了给各个组件的先验概率
* 高斯混合模型是概率密度的万能近似器，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近

## 3.10 常用函数的有用性质
* logistic sigmoid函数在处理概率分布时经常出现
  $${\sigma(x) = \frac{1}{1 + e^{-x}}}$$
  通常用来产生Bernoulli分布中的参数$\phi$
  该函数在变量绝对值较大时会出现饱和现象，对输入的微小变化变得不敏感
* softplus函数
  $${\zeta(x) = log(1 + e^x)}$$
  softplus函数可用来产生正态分布的$\beta$和${\sigma}$参数
* 以下性质需要记下来
  $${\sigma(x) = \frac{e^x}{e^x+e^0}}$$
  $${\frac{d}{dx} \sigma(x) = \sigma(x)(1 - \sigma(x))}$$
  $${1 - \sigma(x) = \sigma(-x)}$$
  $${log \sigma(x) = -\zeta(-x)}$$
  $${\frac{d}{d(x)}\zeta(x) = \sigma(x)}$$
  $${\forall x \in (0,1), \sigma^{-1}(x) = log(\frac{x}{1-x})}$$
  $${\forall x > 0, \zeta^{-1}(x) = log(e^x - 1)}$$
  $${\zeta(x) = \int_{-\infty}^x \sigma(y)dy}$$
  $${\zeta(x) - \zeta(-x) = x}$$

## 3.11 贝叶斯规则
* 贝叶斯规则
  $${P(x|y) = \frac{P(x)P(y|x)}{P(y)}}$$
* ${P(y)}$通常使用下列式子进行计算
  $$P(y) = \sum_x P(y|x)P(x)$$

## 3.12 连续性变量的技术细节
* 测度论
* 雅可比（Jacobian）矩阵

## 3.13 信息论
* 定义一个事件
  $${I(x) = -logP(x)}$$
  log表示自然对数，底数为e  
* I(x)的单位是奈特（nats），一奈特是以${\frac{1}{e}}$的概率观测到一个事件时获得的信息量。
* 使用低数为2的对数，单位是比特（bits）或香农（shannons）
* 自信息只处理单个的输出，可以用香农熵对整个概率分布中的不确定性总量进行量化
  $${H(x) = E_{X\sim P}[I(x)] = -E_{X\sim P}[logP(x)]}$$
* 一个分布的香农熵是指遵循这个分布的时间所产生的期望信息总量，它给出了对依据概率分布P生成的符号进行编码所需的比特数在平均意义上的下界。
* 接近确定性的分布具有较低的熵；接近均匀分布的概率分布具有较高的熵。当x是连续的，香农熵被称为微分熵。
* 对于同一个随机变量X的两个单独的概率分布${P(X)}$和${Q(X)}$，使用KL散度衡量两个分布的差异
  $${D_{KL}(P||Q) = E_{X \sim P}[log \frac{P(x)}{Q(x)}] = E_{X \sim P}[logP(x) - log(Q(x))]}$$
* 在离散型变量的情况下，KL散度衡量的是使用概率分布Q产生的消息的最小编码
* KL散度是非负的
* KL散度为0，当且仅当P和Q在离散型变量的情况下是相同的分布，或者在连续性变量的情况下是“几乎处处”相同的。即KL散度衡量的是两个分布之间的差异，常被用作分布之间的某种距离
* 交叉熵与KL散度很像
  $${H(P,Q) = -E_{X \sim P}logQ(x)}$$
  针对Q最小化交叉熵等价于最小化KL散度

## 3.14 结构化概率模型
* 把概率分布分解为使每个因子分布具有更少变量的分解方法，就能极大降低表示联合分布的成本。
* 当用图来表示这种概率分布的解时，称之为结构化概率模型或者图模型
* 结构化概率模型分为有向的和无向的，图的每个节点对应一个随机变量，连接两个随机变量的边意味着概率分布可以表示成这两个随机变量之间的直接作用。
* 有向模型用条件概率分布来表示分解，对于分布中的每一个随机变量${X_i}$都包含一个影响因子，这个组成${X_i}$条件概率的影响因子被称为${X_i}$的父节点，记作${Pag(X_i)}$
  $${p(X) = \prod_ip(X_i|Pag(X_i))}$$
* 无向模型将分解表示成一组函数，这些函数通常不是任何类型的概率分布。g中满足两两之间有边连接的顶点的集合称为团。无向模型中的每个团$C^{(i)}$都伴随着一个因子${\phi^{(i)}(C^{(i)})}$。这些因子的输出都必须是非负的。
* 随机变量的联合概率与所有这些因子的乘积成比例，因子的值越大，则可能性越大。但不能保证乘积的求和为1，需要除以一个归一化常数Z来得到归一化的概率分布。Z被定义为$\phi$函数乘积的所有状态的求和或积分
  $${p(X) = \frac{1}{Z} \prod_i \phi^{(i)}(C^{(i)})}$$