>书名：深度学习（花书）    
时间：2021年11月9日      
本文标签：深度学习、正则化

# 7 深度学习中的正则化
* 机器学习中的核心问题是设计不仅在训练数据上表现好，而且能在新输入上泛化好的算法，许多策略被显式地设计来减少测试误差，这些策略统称为正则化。

## 7.1 参数范数惩罚
* 许多正则化方法通过对目标函数 J添加一个参数范数惩罚${\Omega(\theta)}$，限制模型的学习能力。将正则化后的目标函数记为${\widetilde J}$
  $${\widetilde{J}(\theta;X,y) = J(\theta;X,y) + \alpha \Omega(\theta)}$$
  其中，${\alpha \in [0,\infty]}$是权衡范数惩罚项和标准目标函数相对贡献的超参数，其值越大对应正则化惩罚越大
* 通常只对权重做惩罚而不对偏置做正则惩罚。正则化偏执参数可能会导致明显的欠拟合。
* 为了减少搜索空间，会在所有层使用相同的权重衰减。

### 7.1.1 ${L^2}$参数正则化
* ${L^2}$参数范数惩罚通常被称为参数衰减，在其他学术圈也被称为岭回归。
* 这个正则化策略通过向目标函数添加一个正则项${\Omega(\theta) = \frac{1}{2}\| \omega \|^2_2}$，使权重更接近原点