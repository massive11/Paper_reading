>书名：深度学习（花书）    
时间：2021年11月1日      
本文标签：深度学习、线性代数  

# 2 线性代数
## 2.1 标量、向量、矩阵和张量
* 张量：一个数组中的元素分布在若干维坐标的规则网格中称为张量。

* 深度学习允许矩阵和向量相加，产生另一个矩阵：**C** = **A** + b，其中
  $$C_{i,j} = A_{i,j} + b_j$$
  即向量b与矩阵A中的每一行相加。这种隐式地复制向量b到很多未知的方式称为**广播**。

## 2.2 矩阵和向量相乘
* 矩阵乘积服从分配律、结合律，但不满足交换律
* 矩阵乘积的转置
  $$(AB)^T = B^TA^T$$

## 2.3 单位矩阵和逆矩阵
* 单位矩阵：所有沿主对角线的元素都是1，而其他位置都是0
* 矩阵的逆称作${A^{-1}}$
  $${A^{-1}A = I_n}$$
  $${AA^{-1} = I_n}$$

## 2.4 线性相关和生成子空间
* 一组向量的 **线性组合** 是指每个向量乘以对应标量系数之后的和，即
  $${\sum_i c_i v^{(i)}}$$
* 一组向量的 **生成子空间（span）** 是原始向量线性组合后所能抵达的点的集合
* 确定${Ax = b}$是否有解，相当于确定向量b是否在A列向量的生成子空间中。这个特殊的生成子空间被称为A的列空间或者A的值域
  <font color=red>（为什么是A列向量不是A行向量？是因为b是列向量吗）</font>
* 线性无关：如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为线性无关。
* 否则，若存在冗余，则称为线性相关。
* 奇异的方阵：列向量线性相关
* 如果矩阵A不是一个方针或是一个奇异的方针，该方阵仍可能有解，但不能用矩阵逆去求解

## 2.5 范数
* 在机器学习中，使用范数来衡量向量大小，${L^p}$范数定义如下
  $${\|x\|_p = (\sum_i \vert x_i \vert^p)^{\frac 1 p}}$$

* 范数是将向量映射到非负值的函数。向量x的范数衡量从原点到x的距离。
* 范数是满足下列性质的任意函数：
  *  $${f(x) = 0 \Rightarrow x = 0}$$
  *  $${f(x+y) \le f(x) + f(y)}$$
  *  $${\forall \alpha \in \mathbb{R}, f(\alpha x) = \vert \alpha \vert f(x)}$$
* 当$p=2$时，称为欧几里得范数，表示从原点出发到向量x确定的点的欧几里得距离，经常简化表示为${\|x\|}$
* 平方${L^2}$范数也常用来衡量向量的大小，可以简单的通过点积${x^Tx}$计算。  
  平方${L^2}$范数比${L^2}$范数本身要更方便，对每个元素的倒数只取决于对应的元素，但是在原点附近增长的十分缓慢，此时可以转用${L^1}$范数
* ${L^1}$范数可以简化如下
  $${\|x\|_1 = \sum_i \vert x_i \vert}$$
  通常当机器学习问题中0和非0元素之间的差异非常重要时使用。
* ${L^{\infty}}$范数也称最大范数，表示向量中具有最大幅值的元素的绝对值
  $${\|x\|_\infty = max_i \vert x_i \vert}$$
* 衡量矩阵的大小使用Frobenius范数（类似向量的${L^2}$范数）
  $$\|A\|_F = \sqrt {\sum_{ij} A^2_{i,j}}$$
* 两个向量的点积可以用范数来表示
  $${x^Ty = \|x\|_2 \|y\|_2 cos \theta}$$

## 2.6 特殊类型的矩阵和向量
* 对角矩阵：只要主对角线上含有非零元素，其他位置都是零。用$diag(v)$表示对角元素由向量v中元素给定的一个对角方阵。
* 对角方阵的逆矩阵存在，当且仅当对角元素都是非零值